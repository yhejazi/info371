{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO371 Final Exam/Winter 2019\n",
    "Score: 20/20\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is the final exam. As you can see, it is heavily influenced from PS4 and 6. Rules:\n",
    "- You may use all available materials, including internet sources\n",
    "- You must not communicate with the others. This is an individual exam.\n",
    "- References must be appropriately cited. Links to solutions copied from websites, such as StackOverflow must be provided in code comments. If you are re-using code from your previous problem sets, mention it.\n",
    "- You have to sign the compliance form, attached on the last page, and return it together with the rest of the exam (a cellphone photo of the signed form is OK).\n",
    "- Please explain your answers and show all your work; a complete argument must be presented to obtain full credit.\n",
    "- All plots must be appropriately labeled, and appropriate colors/labels/font sizes must be used. Here I mean appropriate for thYs exam, it does not conform to the looong explanations in Science journal.\n",
    "- You can instructors if you are stuck with something. I want you to spend most of the time on ML problems, not on an obscure technical issue. However, it's somewhat limited how much help we can offer.\n",
    "The exam contains 2 questions. The first one uses Boston Housing data, the second one is about text classification of newgroup articles (and the data is probably new to you). I expect you to use existing packages and not implement any code yourself. But you are welcome to, and you are also welcome to explore other avenues besides strictly asked below.\n",
    "As always, please submit your results in two ways: as a code (notebook/markdown/...) and as a final version (html/pdf/...).\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression (10pt)\n",
    "\n",
    "Your first task is to demonstrate your skills on the good old (and boring) Boston housing data. You estimate Boston house prices (medv) using all other features as predictors. Use cross-validation to establish the best predictor, we don't spend time on a separate testing data here.\n",
    "I expect you to use existing packages and functions, not to implement the methods yourself. However, feel free implement your own, or to re-use something you have done for a previous problem set.\n",
    "\n",
    "1) Load Boston data. It's located in R packages MASS (just use MASS::Boston), and in sklearn where you can load it as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "bdata = load_boston()\n",
    "X = bdata.data\n",
    "y = bdata.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: dict_keys(['data', 'target', 'feature_names', 'DESCR'])\n",
      "\n",
      "feature names: ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "\n",
      "data shape: (506, 13)\n",
      "\n",
      "target shape: (506,)\n"
     ]
    }
   ],
   "source": [
    "# check also boston.feature_names and other information\n",
    "print('keys:', bdata.keys())\n",
    "print('\\nfeature names:', bdata.feature_names)\n",
    "print('\\ndata shape:', bdata.data.shape)\n",
    "print('\\ntarget shape:', bdata.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that target MEDV is separated from data, and all features are numeric (this is already the case if you use sklearn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston = pd.DataFrame(bdata.data)\n",
    "boston.columns = bdata.feature_names[:]\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRIM       float64\n",
      "ZN         float64\n",
      "INDUS      float64\n",
      "CHAS       float64\n",
      "NOX        float64\n",
      "RM         float64\n",
      "AGE        float64\n",
      "DIS        float64\n",
      "RAD        float64\n",
      "TAX        float64\n",
      "PTRATIO    float64\n",
      "B          float64\n",
      "LSTAT      float64\n",
      "dtype: object \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Are all features numeric?\n",
    "print(boston.dtypes, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) List all relevant methods you know (and we covered in the class) for predicting MEDV based on this data. In each case also discuss the strenghs and weaknesses of the methods, and list the most important hyperparameters you may consider (more complex methods may have very large number of hyperparameters, so you don't want to list them all). Also, do not consider the optimizer and related tolerances as a hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**kNN**:\n",
    "\n",
    "Important hyperparameters: The k in k-nearest neighbors.\n",
    "\n",
    "Strengths: simple and surprisingly flexible if you pre-process your data and engineer useful features.\n",
    "\n",
    "Weaknesses: you need to specify the number of neighbors/clusters, which isn't always easy to do\n",
    "\n",
    "**Logistic**:\n",
    "\n",
    "Important hyperparameters: The penalty and C in logistic.\n",
    "\n",
    "Strengths: algorithm can be regularized to avoid overfitting, nice probabilistic interpretation\n",
    "\n",
    "Weaknesses: tends to underperform when there are multiple or non-linear decision boundaries, can't catch complexity\n",
    "\n",
    "**Lasso**:\n",
    "\n",
    "Important hyperparameters: alpha\n",
    "\n",
    "Strengths: majorly used to prevent overfitting, sets less important predictors to zero and helps you with choosing the predictors that can be left out of the model\n",
    "\n",
    "Weaknesses: In many analyses, it can be important to include *all* variables and not drop the ones that do not have significance in the predicting. They could be necessary control variables or their small effect could hold a large significance\n",
    "\n",
    "**Ridge**:\n",
    "\n",
    "Important hyperparameters:: alpha\n",
    "\n",
    "Strengths: majorly used to prevent overfitting by adding a penalty to models that have too large coefficients \n",
    "\n",
    "Weaknesses: Since it includes all the features, it is not very useful in case of exorbitantly high features, say in millions, as it will pose computational challenges.\n",
    "\n",
    "**OLS**:\n",
    "\n",
    "Important hyperparameters: there is none. The number/choice of features is not a hyperparameter, but can be viewed as a post processing or iterative tuning process.\n",
    "\n",
    "Strengths: can be regularized to avoid overfitting, very useful on a huge amount of features where better algorithms suffer from overfitting\n",
    "\n",
    "Weaknesses: performs poorly when there are non-linear relationships (can't catch complexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Use at least three of the methods you listed to predict MEDV based on other attributes. Ideas: OLS, k-NN, trees, Lasso, compare normalized-versus non-normalized features, try different distance metrics.\n",
    "\n",
    "In each case:\n",
    "\n",
    "(a) select a relevant measure of goodness, such as RMSE or R2. \n",
    "\n",
    "(b) loop over selected hyperparameters.\n",
    "\n",
    "(c) cross-validate your result using the selected goodness measure and pick the best set of hyper- parameters.\n",
    "\n",
    "4) Present your results a table, or a graph, or another easily accessible summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "k = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "scores = {'k': [], 'score': []}\n",
    "\n",
    "for i in k:\n",
    "    neigh = KNeighborsRegressor(n_neighbors=i).fit(X_train, y_train)\n",
    "    scores['k'].append(i)\n",
    "    scores['score'].append(neigh.score(X_val, y_val))\n",
    "\n",
    "scores = pd.DataFrame(scores)   \n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a12805438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(scores['k'], scores['score'], c = 'red')\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"R2\")\n",
    "plt.title(\"kNN\")\n",
    "plt.ylim((0, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.767482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.783993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.781521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.775130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.767020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.758054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.748680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.739154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.729632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.720208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.710942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alpha     score\n",
       "0     0.0  0.767482\n",
       "1     0.1  0.783993\n",
       "2     0.2  0.781521\n",
       "3     0.3  0.775130\n",
       "4     0.4  0.767020\n",
       "5     0.5  0.758054\n",
       "6     0.6  0.748680\n",
       "7     0.7  0.739154\n",
       "8     0.8  0.729632\n",
       "9     0.9  0.720208\n",
       "10    1.0  0.710942"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "alphas = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "scores = {'alpha': [], 'score': []}\n",
    "\n",
    "for i in alphas:\n",
    "    ridgereg = Ridge(alpha = i, normalize=True)\n",
    "    ridgereg.fit(X_train, y_train)\n",
    "    scores['alpha'].append(i)\n",
    "    scores['score'].append(ridgereg.score(X_val, y_val))\n",
    "    \n",
    "scores = pd.DataFrame(scores)   \n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a12cc9ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(scores['alpha'], scores['score'], c = 'purple')\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"R2\")\n",
    "plt.title(\"ridge\")\n",
    "plt.ylim((0, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yasmine/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  \n",
      "/Users/yasmine/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:477: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  positive)\n",
      "/Users/yasmine/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.740608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.582783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.341435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.004698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alpha     score\n",
       "0     0.0  0.740608\n",
       "1     0.1  0.582783\n",
       "2     0.2  0.341435\n",
       "3     0.3  0.004698\n",
       "4     0.4  0.000000\n",
       "5     0.5  0.000000\n",
       "6     0.6  0.000000\n",
       "7     0.7  0.000000\n",
       "8     0.8  0.000000\n",
       "9     0.9  0.000000\n",
       "10    1.0  0.000000"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "scores = {'alpha': [], 'score': []}\n",
    "for i in alphas:\n",
    "    lassoreg = Lasso(alpha = i, normalize=True)\n",
    "    lassoreg.fit(X_train, y_train)\n",
    "    scores['alpha'].append(i)\n",
    "    scores['score'].append(lassoreg.score(X_val, y_val))\n",
    "    \n",
    "scores = pd.DataFrame(scores)   \n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a12eb3780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(scores['alpha'], scores['score'], c = 'blue')\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.ylabel(\"R2\")\n",
    "plt.title(\"lasso\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Comment your results. Which methods performed best? Can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cross-validation I performed, I got that Ridge was the best model of the 3, with alpha = 1. We see better performance than Lasso and knn as well with different alphas for Ridge. The best R^2 Coeff we get with alpha = 1 is .784. This may be because ridge is able to normalize while including all the features. kNN performed better before cross-validation, and it performed better when I did different cross validations. This can be because there's not much overlap in the data, so kNN with fewer neighbors can easily pick this up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification (10p)\n",
    "\n",
    "This question is a compilation of your PS4 and PS6. Your task is to categorize texts into newgroups (there are 20 newsgroups) using k-NN and Naive Bayes methods. I expect you to use the existing stock libraries but you are welcome to amend what you implemented yourself. We follow the testing data approach, so you are supposed to split the data, keep testing data well away, and at the end report the final performance.\n",
    "The data is from UCI ML library and you can read a brief description there. Note that it includes many traits that make it unfairly nice for NB compared to k-NN. I have done my best to strip it from sensitive information, but it still contains a large number of names, phone numbers, etc, that may hint very well which newsgroup a text belongs to. The cleaned data is converted into a csv file with two attributes: group is the name of the newsgroup, and text is the text itself. There are 20k texts in the sample.\n",
    "Tasks:\n",
    "\n",
    "1) Load the data and save a random subsample ( 20%) of it away as testing data. Note: if you know that your computer is slow and you struggled with the data size in PS4 and PS6, you may as well store (say) 75% of it as testing data, or just work on a much smaller sample. On my old desktop, the full run of all models takes  5 mins and no more than 3G of RAM.\n",
    "\n",
    "In any case, if you cut your sample, please state it clearly along the lines: I chose to work on a subsample of 5000 texts only.\n",
    "\n",
    "Hint: *CountVectorizer* returns a sparse matrix. Keep it sparse. You save a lot of memory (and improve speed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>Atheist Resources  Addresses of Atheist Organ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>--BEGIN PGP SIGNED MESSAGE--  An Introduction ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>In article   (Charley Wingate) writes: Well, J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>(..until kings become philosophers or philoso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>In article (Bob McGwier) writes:  [1] HOWEVER,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         group                                               text\n",
       "0  alt.atheism   Atheist Resources  Addresses of Atheist Organ...\n",
       "1  alt.atheism  --BEGIN PGP SIGNED MESSAGE--  An Introduction ...\n",
       "2  alt.atheism  In article   (Charley Wingate) writes: Well, J...\n",
       "3  alt.atheism   (..until kings become philosophers or philoso...\n",
       "4  alt.atheism  In article (Bob McGwier) writes:  [1] HOWEVER,..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups = pd.read_csv(\"20-newsgroups.csv\")\n",
    "newsgroups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups.text = newsgroups.text.astype(str)\n",
    "newsgroups.group = newsgroups.group.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(newsgroups, test_size=0.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.to_csv(path_or_buf='test.csv', index = False)\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9998"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Convert the texts into bag-of-words (no TF-IDF here) and use k-NN with cosine similarity for categorization. Find the best k through CV while trying to maximize accuracy.\n",
    "\n",
    "Hints: sklearn's KNeighborsClassifier does not mention cosine metrics. However, it works for me (and is mentioned under sklearn.neighbors.NearestNeighbors).\n",
    "\n",
    "When you use pd.read_csv, the text may end up of type object and confuse the vectorizer and other text functions. Convert it to str type with something like string = text.astype('str')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train[['text']]\n",
    "y_train = train[['group']]\n",
    "param = range(1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data_frame(df, num_chunks): \n",
    "    listOfDf = list()\n",
    "    chunk_size = len(df) // num_chunks\n",
    "    for i in range(num_chunks):\n",
    "        listOfDf.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "    return listOfDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val(data, k_chunks, k):\n",
    "    print('Number of chunks:', k_chunks)\n",
    "    print('k:', k)\n",
    "    \n",
    "    chunks = split_data_frame(data, k_chunks)\n",
    "    accuracy_list = []\n",
    "    for chunk_df in chunks:\n",
    "        tmp_train = data[~data.isin(chunk_df)]\n",
    "        X_train = tmp_train.text.astype(str)\n",
    "        y = tmp_train.group.astype(str)\n",
    "\n",
    "        y_output = chunk_df.group\n",
    "        \n",
    "        # initialize the vectorizer\n",
    "        vectorizer = CountVectorizer(min_df=0)\n",
    "        # create the dictionary\n",
    "        vectorizer.fit(X_train)\n",
    "        # `fit` builds the vocabulary\n",
    "        # transform your data into the BOW array\n",
    "        X = vectorizer.transform(X_train).toarray()\n",
    "        words_list = list(vectorizer.get_feature_names())\n",
    "    \n",
    "        # fit knn\n",
    "        neigh = KNeighborsClassifier(n_neighbors = k)\n",
    "        neigh.fit(X, y_train)\n",
    "        \n",
    "        \n",
    "        X_val = vectorizer.transform(chunk_df.text)\n",
    "        predicted = neigh.predict(X_val)\n",
    "\n",
    "        accuracy = metrics.accuracy_score(y_output, predicted) * 100\n",
    "        accuracy_list.append(accuracy)\n",
    "       \n",
    "        print('Accuracy:', accuracy)\n",
    "    print('Average accuracy:', np.mean(accuracy_list))\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 5\n",
      "k: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yasmine/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 37.8189094547\n",
      "Accuracy: 36.8184092046\n",
      "Accuracy: 37.0185092546\n",
      "Accuracy: 38.2191095548\n",
      "Accuracy: 36.4182091046\n",
      "Average accuracy: 37.2586293147\n",
      "------\n",
      "Time elapsed: 420.0792419910431\n",
      "Number of chunks: 5\n",
      "k: 2\n",
      "Accuracy: 33.0665332666\n",
      "Accuracy: 31.715857929\n",
      "Accuracy: 32.9664832416\n",
      "Accuracy: 33.1165582791\n",
      "Accuracy: 32.316158079\n",
      "Average accuracy: 32.6363181591\n",
      "------\n",
      "Time elapsed: 423.1376988887787\n",
      "Number of chunks: 5\n",
      "k: 3\n",
      "Accuracy: 31.6658329165\n",
      "Accuracy: 30.5152576288\n",
      "Accuracy: 31.2156078039\n",
      "Accuracy: 32.116058029\n",
      "Accuracy: 31.1655827914\n",
      "Average accuracy: 31.3356678339\n",
      "------\n",
      "Time elapsed: 410.5582060813904\n",
      "Number of chunks: 5\n",
      "k: 4\n",
      "Accuracy: 30.1650825413\n",
      "Accuracy: 28.7643821911\n",
      "Accuracy: 29.9649824912\n",
      "Accuracy: 31.9659829915\n",
      "Accuracy: 30.4652326163\n",
      "Average accuracy: 30.2651325663\n",
      "------\n",
      "Time elapsed: 426.17114996910095\n",
      "Number of chunks: 5\n",
      "k: 5\n",
      "Accuracy: 29.9149574787\n",
      "Accuracy: 28.5142571286\n",
      "Accuracy: 29.8649324662\n",
      "Accuracy: 30.9154577289\n",
      "Accuracy: 30.4152076038\n",
      "Average accuracy: 29.9249624812\n",
      "------\n",
      "Time elapsed: 401.02513790130615\n",
      "Number of chunks: 5\n",
      "k: 6\n",
      "Accuracy: 29.9649824912\n",
      "Accuracy: 28.6143071536\n",
      "Accuracy: 29.6648324162\n",
      "Accuracy: 30.5152576288\n",
      "Accuracy: 30.2651325663\n",
      "Average accuracy: 29.8049024512\n",
      "------\n",
      "Time elapsed: 400.12249207496643\n",
      "Number of chunks: 5\n",
      "k: 7\n",
      "Accuracy: 29.5647823912\n",
      "Accuracy: 29.1645822911\n",
      "Accuracy: 29.9149574787\n",
      "Accuracy: 29.4147073537\n",
      "Accuracy: 29.7648824412\n",
      "Average accuracy: 29.5647823912\n",
      "------\n",
      "Time elapsed: 390.2418098449707\n",
      "Number of chunks: 5\n",
      "k: 8\n",
      "Accuracy: 29.4647323662\n",
      "Accuracy: 28.364182091\n",
      "Accuracy: 29.5647823912\n",
      "Accuracy: 29.6648324162\n",
      "Accuracy: 28.8644322161\n",
      "Average accuracy: 29.1845922961\n",
      "------\n",
      "Time elapsed: 437.49167013168335\n",
      "Number of chunks: 5\n",
      "k: 9\n",
      "Accuracy: 30.1150575288\n",
      "Accuracy: 27.8139069535\n",
      "Accuracy: 29.6648324162\n",
      "Accuracy: 29.3646823412\n",
      "Accuracy: 29.0145072536\n",
      "Average accuracy: 29.1945972986\n",
      "------\n",
      "Time elapsed: 448.59814620018005\n",
      "Number of chunks: 5\n",
      "k: 10\n",
      "Accuracy: 29.7148574287\n",
      "Accuracy: 28.1140570285\n",
      "Accuracy: 29.1645822911\n",
      "Accuracy: 28.8144072036\n",
      "Accuracy: 28.8144072036\n",
      "Average accuracy: 28.9244622311\n",
      "------\n",
      "Time elapsed: 421.8404767513275\n"
     ]
    }
   ],
   "source": [
    "for t in param:\n",
    "    start = time.time()\n",
    "    cross_val(train, 5, t)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Now repeat the exact same exercise with TF-IDF. I expect you to use TF-IDF vectorizer, but you are welcome to use your own implementation. Note, however, that sklearn-s TF-IDF definition differs slightly from Murphy's, the one we used in PS4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_cross_val(data, k_chunks, k):\n",
    "    print('Number of chunks:', k_chunks)\n",
    "    print('k:', k)\n",
    "    \n",
    "    chunks = split_data_frame(data, k_chunks)\n",
    "    accuracy_list = []\n",
    "    for chunk_df in chunks:\n",
    "        tmp_train = data[~data.isin(chunk_df)]\n",
    "        X_train = tmp_train.text.astype(str)\n",
    "        y = tmp_train.group.astype(str)\n",
    "\n",
    "        y_output = chunk_df.group\n",
    "        \n",
    "        # initialize the vectorizer\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        # create the dictionary\n",
    "        vectorizer.fit(X_train)\n",
    "        # `fit` builds the vocabulary\n",
    "        # transform your data into the BOW array\n",
    "        X = vectorizer.transform(X_train).toarray()\n",
    "        words_list = list(vectorizer.get_feature_names())\n",
    "    \n",
    "        # fit knn\n",
    "        neigh = KNeighborsClassifier(n_neighbors = k)\n",
    "        neigh.fit(X, y_train)\n",
    "        \n",
    "        \n",
    "        X_val = vectorizer.transform(chunk_df.text)\n",
    "        predicted = neigh.predict(X_val)\n",
    "\n",
    "        accuracy = metrics.accuracy_score(y_output, predicted) * 100\n",
    "        accuracy_list.append(accuracy)\n",
    "       \n",
    "        print('Accuracy:', accuracy)\n",
    "    print('Average accuracy:', np.mean(accuracy_list))\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 5\n",
      "k: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yasmine/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.136068034\n",
      "Accuracy: 71.2356178089\n",
      "Accuracy: 70.6353176588\n",
      "Accuracy: 71.7858929465\n",
      "Accuracy: 70.4852426213\n",
      "Average accuracy: 71.2556278139\n",
      "------\n",
      "Time elapsed: 398.2660291194916\n",
      "Number of chunks: 5\n",
      "k: 2\n",
      "Accuracy: 68.7843921961\n",
      "Accuracy: 67.6338169085\n",
      "Accuracy: 67.1835917959\n",
      "Accuracy: 69.4847423712\n",
      "Accuracy: 64.9324662331\n",
      "Average accuracy: 67.603801901\n",
      "------\n",
      "Time elapsed: 383.16657519340515\n",
      "Number of chunks: 5\n",
      "k: 3\n",
      "Accuracy: 68.9344672336\n",
      "Accuracy: 67.4337168584\n",
      "Accuracy: 66.4332166083\n",
      "Accuracy: 69.9849924962\n",
      "Accuracy: 65.5327663832\n",
      "Average accuracy: 67.663831916\n",
      "------\n",
      "Time elapsed: 389.0017430782318\n",
      "Number of chunks: 5\n",
      "k: 4\n",
      "Accuracy: 68.2341170585\n",
      "Accuracy: 66.9334667334\n",
      "Accuracy: 65.2826413207\n",
      "Accuracy: 68.384192096\n",
      "Accuracy: 64.4822411206\n",
      "Average accuracy: 66.6633316658\n",
      "------\n",
      "Time elapsed: 381.80662298202515\n",
      "Number of chunks: 5\n",
      "k: 5\n",
      "Accuracy: 67.3336668334\n",
      "Accuracy: 66.7333666833\n",
      "Accuracy: 65.6828414207\n",
      "Accuracy: 68.4842421211\n",
      "Accuracy: 64.5822911456\n",
      "Average accuracy: 66.5632816408\n",
      "------\n",
      "Time elapsed: 389.20574021339417\n",
      "Number of chunks: 5\n",
      "k: 6\n",
      "Accuracy: 66.7833916958\n",
      "Accuracy: 66.5832916458\n",
      "Accuracy: 65.6328164082\n",
      "Accuracy: 67.1335667834\n",
      "Accuracy: 65.1325662831\n",
      "Average accuracy: 66.2531265633\n",
      "------\n",
      "Time elapsed: 383.60361886024475\n",
      "Number of chunks: 5\n",
      "k: 7\n",
      "Accuracy: 66.4832416208\n",
      "Accuracy: 66.8834417209\n",
      "Accuracy: 64.132066033\n",
      "Accuracy: 67.3336668334\n",
      "Accuracy: 65.1825912956\n",
      "Average accuracy: 66.0030015008\n",
      "------\n",
      "Time elapsed: 382.8220419883728\n",
      "Number of chunks: 5\n",
      "k: 8\n",
      "Accuracy: 66.5332666333\n",
      "Accuracy: 66.0830415208\n",
      "Accuracy: 63.831915958\n",
      "Accuracy: 66.2831415708\n",
      "Accuracy: 64.8324162081\n",
      "Average accuracy: 65.5127563782\n",
      "------\n",
      "Time elapsed: 434.5470998287201\n",
      "Number of chunks: 5\n",
      "k: 9\n",
      "Accuracy: 65.2326163082\n",
      "Accuracy: 65.2826413207\n",
      "Accuracy: 63.3816908454\n",
      "Accuracy: 65.8829414707\n",
      "Accuracy: 62.6813406703\n",
      "Average accuracy: 64.4922461231\n",
      "------\n",
      "Time elapsed: 420.3932981491089\n",
      "Number of chunks: 5\n",
      "k: 10\n",
      "Accuracy: 65.0325162581\n",
      "Accuracy: 65.2826413207\n",
      "Accuracy: 63.0315157579\n",
      "Accuracy: 65.5327663832\n",
      "Accuracy: 62.6813406703\n",
      "Average accuracy: 64.312156078\n",
      "------\n",
      "Time elapsed: 432.011696100235\n"
     ]
    }
   ],
   "source": [
    "for t in param:\n",
    "    start = time.time()\n",
    "    tfidf_cross_val(train, 5, t)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Third, repeat the above using Naive Bayes. Let's stay with the BOW representation where we have word counts (unless you explicitly requested something else, you already have it from the first k-NN). Here you should nd the optimal smoothing parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nb_cross_val(data, k_chunks, k):\n",
    "    print('Number of chunks:', k_chunks)\n",
    "    print('alpha:', k)\n",
    "    \n",
    "    chunks = split_data_frame(data, k_chunks)\n",
    "    accuracy_list = []\n",
    "    for chunk_df in chunks:\n",
    "        tmp_train = data[~data.isin(chunk_df)]\n",
    "        X_train = tmp_train.text.astype(str)\n",
    "        y = tmp_train.group.astype(str)\n",
    "\n",
    "        y_output = chunk_df.group\n",
    "        \n",
    "        # initialize the vectorizer\n",
    "        vectorizer = CountVectorizer(min_df=0)\n",
    "        # create the dictionary\n",
    "        vectorizer.fit(X_train)\n",
    "        # `fit` builds the vocabulary\n",
    "        # transform your data into the BOW array\n",
    "        X = vectorizer.transform(X_train).toarray()\n",
    "        words_list = list(vectorizer.get_feature_names())\n",
    "    \n",
    "        # fit nb\n",
    "        nb = MultinomialNB(alpha = k)\n",
    "        nb.fit(X, y_train)\n",
    "        \n",
    "        \n",
    "        X_val = vectorizer.transform(chunk_df.text)\n",
    "        predicted = nb.predict(X_val)\n",
    "\n",
    "        accuracy = metrics.accuracy_score(y_output, predicted) * 100\n",
    "        accuracy_list.append(accuracy)\n",
    "       \n",
    "        print('Accuracy:', accuracy)\n",
    "    print('Average accuracy:', np.mean(accuracy_list))\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 5\n",
      "alpha: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yasmine/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.047891351\n",
      "Accuracy: 63.9027877055\n",
      "Accuracy: 68.1915654039\n",
      "Accuracy: 66.6904932094\n",
      "Accuracy: 64.0457469621\n",
      "Average accuracy: 65.9756969264\n",
      "------\n",
      "Time elapsed: 104.5196008682251\n",
      "Number of chunks: 5\n",
      "alpha: 2\n",
      "Accuracy: 52.3230879199\n",
      "Accuracy: 48.820586133\n",
      "Accuracy: 55.5396711937\n",
      "Accuracy: 54.896354539\n",
      "Accuracy: 49.3924231594\n",
      "Average accuracy: 52.194424589\n",
      "------\n",
      "Time elapsed: 104.20132398605347\n",
      "Number of chunks: 5\n",
      "alpha: 3\n",
      "Accuracy: 43.1736954968\n",
      "Accuracy: 40.2430307362\n",
      "Accuracy: 48.3917083631\n",
      "Accuracy: 46.5332380272\n",
      "Accuracy: 41.815582559\n",
      "Average accuracy: 44.0314510365\n",
      "------\n",
      "Time elapsed: 102.9996452331543\n",
      "Number of chunks: 5\n",
      "alpha: 4\n",
      "Accuracy: 37.2408863474\n",
      "Accuracy: 34.5961401001\n",
      "Accuracy: 43.1736954968\n",
      "Accuracy: 40.1000714796\n",
      "Accuracy: 36.8834882059\n",
      "Average accuracy: 38.3988563259\n",
      "------\n",
      "Time elapsed: 103.53547286987305\n",
      "Number of chunks: 5\n",
      "alpha: 5\n",
      "Accuracy: 33.5239456755\n",
      "Accuracy: 31.6654753395\n",
      "Accuracy: 40.1000714796\n",
      "Accuracy: 35.9542530379\n",
      "Accuracy: 32.6661901358\n",
      "Average accuracy: 34.7819871337\n",
      "------\n",
      "Time elapsed: 106.79887294769287\n",
      "Number of chunks: 5\n",
      "alpha: 6\n",
      "Accuracy: 30.9506790565\n",
      "Accuracy: 29.0922087205\n",
      "Accuracy: 37.4553252323\n",
      "Accuracy: 33.6669049321\n",
      "Accuracy: 30.5932809149\n",
      "Average accuracy: 32.3516797713\n",
      "------\n",
      "Time elapsed: 111.79115295410156\n",
      "Number of chunks: 5\n",
      "alpha: 7\n",
      "Accuracy: 29.3781272337\n",
      "Accuracy: 27.0907791279\n",
      "Accuracy: 34.2387419585\n",
      "Accuracy: 31.0936383131\n",
      "Accuracy: 29.0922087205\n",
      "Average accuracy: 30.1786990708\n",
      "------\n",
      "Time elapsed: 102.84812116622925\n",
      "Number of chunks: 5\n",
      "alpha: 8\n",
      "Accuracy: 27.1622587563\n",
      "Accuracy: 25.3752680486\n",
      "Accuracy: 32.8806290207\n",
      "Accuracy: 28.9492494639\n",
      "Accuracy: 27.5911365261\n",
      "Average accuracy: 28.3917083631\n",
      "------\n",
      "Time elapsed: 106.15774393081665\n",
      "Number of chunks: 5\n",
      "alpha: 9\n",
      "Accuracy: 25.1608291637\n",
      "Accuracy: 23.8741958542\n",
      "Accuracy: 31.1651179414\n",
      "Accuracy: 27.0192994996\n",
      "Accuracy: 26.5189421015\n",
      "Average accuracy: 26.7476769121\n",
      "------\n",
      "Time elapsed: 105.95693278312683\n",
      "Number of chunks: 5\n",
      "alpha: 10\n",
      "Accuracy: 24.0886347391\n",
      "Accuracy: 23.1593995711\n",
      "Accuracy: 29.0922087205\n",
      "Accuracy: 26.0185847034\n",
      "Accuracy: 25.9471050751\n",
      "Average accuracy: 25.6611865618\n",
      "------\n",
      "Time elapsed: 100.29582715034485\n"
     ]
    }
   ],
   "source": [
    "for t in param:\n",
    "    start = time.time()\n",
    "    nb_cross_val(train, 5, t)\n",
    "    end = time.time()\n",
    "    print(\"Time elapsed:\", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Discuss the findings. Which model showed the best behaviour? Which one has the fastest speed? Decide which one is your best model for the last step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The model that showed the best behavior was **TF-IDF kNN**, with an k of k=1. The best average accuracy score we received with this model while implementing k-fold CV with 5 k-chunks was **71.26**. The model that has the fastest speed from our performance was **naive bayes** (average speed = ~103 seconds). For the last step, we will use the former model.\n",
    "\n",
    "(Disclaimer: for the speed, I had to lower the training set size for the naive bayes model because I was running out of time for this final. The previous 2 took over an hour each to run, as shown in the times above, so I couldn't wait another hour to finish this by the deadline. My best guess is that naive bayes would still be the fastest model, just by looking at previous problem sets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Finally, analyze you model's performance on the testing data. load your testing data and compute the final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "test.text = test.text.astype(str)\n",
    "test.group = test.group.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 41.5341534153\n"
     ]
    }
   ],
   "source": [
    "X_train = train.text\n",
    "y_train = train.group\n",
    "\n",
    "# initialize the vectorizer\n",
    "vectorizer = CountVectorizer(min_df=0)\n",
    "# create the dictionary\n",
    "vectorizer.fit(X_train)\n",
    "# `fit` builds the vocabulary\n",
    "# transform your data into the BOW array\n",
    "X = vectorizer.transform(X_train).toarray()\n",
    "\n",
    "# fit knn\n",
    "neigh = KNeighborsClassifier(n_neighbors = 1, algorithm = 'brute')\n",
    "neigh.fit(X, y_train)\n",
    "\n",
    "X_test = vectorizer.transform(test.text)\n",
    "y_test = test.group\n",
    "predicted = neigh.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, predicted) * 100\n",
    "print('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, even though we got a good accuracy like explained above, the accuracy for the final model was really bad. :( With more time, I could play around with this and see what else I could get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Besides the final accuracy, compare some of the misclassified cases. What do you think what and why did the model get these wrong?\n",
    "\n",
    "Note: I mean some sort of informal list of some of the misclassified cases, not a comprehensive analysis. Confusion matrix may not work well as we have 20 different groups here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>alt.atheism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>rec.sport.baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>sci.crypt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>sci.electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>talk.politics.misc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0\n",
       "304               alt.atheism\n",
       "342             comp.graphics\n",
       "304   comp.os.ms-windows.misc\n",
       "330  comp.sys.ibm.pc.hardware\n",
       "319     comp.sys.mac.hardware\n",
       "310            comp.windows.x\n",
       "321              misc.forsale\n",
       "313                 rec.autos\n",
       "291           rec.motorcycles\n",
       "265        rec.sport.baseball\n",
       "229          rec.sport.hockey\n",
       "217                 sci.crypt\n",
       "331           sci.electronics\n",
       "285                   sci.med\n",
       "278                 sci.space\n",
       "315    soc.religion.christian\n",
       "261        talk.politics.guns\n",
       "225     talk.politics.mideast\n",
       "289        talk.politics.misc\n",
       "317        talk.religion.misc"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misc = np.array(y_test)[p.where(y_test != predicted)]\n",
    "unique, counts = np.unique(misc, return_counts = True)\n",
    "pd.DataFrame(unique, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most misclassified group was **comp.graphics** from the model and data we tested on. This may be because there is not a lot of cases where this group appears in the data, or because of this there is not a lot for the model to use to train data to be predicted correctly to this group. Perhaps some of the training data for this group overlapped a lot with or were similar to other groups (perhaps other groups in the same *'comp'* prefix?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
