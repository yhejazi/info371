{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 5: Logistic Regression, SVM, Classification\n",
    "\n",
    "Yasmine Hejazi\n",
    "\n",
    "Due: Tue, Mach 5 midnight\n",
    "\n",
    "Collaborators:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This problem set is long but I hope it is still doable :-) It revolves around classi􏰀cation, k-NN, logistic regression, and support vector machines. At the end you are asked to construct a ROC curve.\n",
    "\n",
    "\n",
    "Please submit \n",
    "\n",
    "a) your code (notebooks, rmd, whatever) and \n",
    "\n",
    "b) the results in a 􏰀nal output form (html or pdf). \n",
    "\n",
    "You are free to choose either R or python for solving this problem set. In case of python, scikit- learn includes all the functionality. In case of R, the funcitonality is spread among di􏰅erent packages, I have had good experience with FNN for k-NN, e1071 for svm (but liquidSVM is supposed to be better), logistic regression you can do using the base functionality. Most of the problems are about employing existing libraries and making plots. But two things you have to implement yourself: one is computing accuracy􏰃precision􏰃recall, and the other is creating a ROC curve.\n",
    "\n",
    "You are welcome to answer some of the questions on paper but please include the result as an image in your 􏰀nal 􏰀le. Note that you can easily include images in both notebooks and .rmd􏰄besides of the code, both are just markdown documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wisconsin Breast Cancer Dataset\n",
    "\n",
    "You will work with Wisconsin Breast Cancer Dataset (WBCD), available at UCI Machine Learning Repository. You can download it from the internet but rather use the 􏰀les wdbc.csv.bz2 and wdbc_doc.txt from canvas (under 􏰀les/data) where I have added the variable names to the data. The 􏰀rst 􏰀le is the csv with variable names, the second one a brief description of the data.\n",
    "\n",
    "The data includes diagnosis of the tumor with 􏰁M􏰂 meaning cancer (malignant) and 􏰁B􏰂 no cancer (benign), and 10 features, describing physical properties of cell nuclei from biopsy samples. Each feature is represented three times, once for mean, once for standard error, and once for the worst values. Your task is to predict diagnosis based on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore the Data\n",
    "\n",
    "As the fi􏰀rst step, explore the data.\n",
    "\n",
    "1. Load the data. You may drop id or just ignore it in the rest of your analysis.\n",
    "2. Create a summary table where you show means, ranges, and number of missings for each variable. In addition, add (Pearson) correlation between the diagnosis and the corresponding feature. You may include more statistics you consider useful.\n",
    "3. Graphical exploration. Make a number of scatterplots where you explore the relationship between features and the diagnosis.\n",
    "\n",
    "Note: you may attempt to display all 435 possible combinations as a scatterplot matrix, but that will most likely be just unreadable. Choose instead a few cases with high correlation and a few with medium/low correlation. Avoid overwhelming your reader with tens of similar 􏰀gures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Boundary\n",
    "\n",
    "Your next task is to plot the decision boundary when using logistic regression. You will also play a little with feature engineering.\n",
    "\n",
    "If you are uncertain about what is decision boundary, I recommend to consult James et al. (2015) book Section 2. For instance, Figure 2.13 on p38 depicts a 2D classi􏰀cation case where certain X1, X2 values are classi􏰀ed as orange and others as blue. Decision boundary is the dashed winding line that separates these two regions.\n",
    "\n",
    "There are two broad strategies to plot it. In any case, you have 􏰀rst to estimate (train) your model. Thereafter you have to predict the classes (cancer/no cancer here) on a regular dense grid that covers the parameter space (this is the small blue/orange dots on 􏰀gure 2.13). Afterwards you can either plot your predicted values with a certain color code, or alternatively, say, set predicted M to 1 and predicted B to 0, and make a contour plot for a single contour at level 0.5. You may also combine these both methods. Some example code in R is provided here, python version will be in a separate 􏰀le.\n",
    "\n",
    "We ignore training/testing/over􏰀tting issues for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 kNN Case\n",
    "\n",
    "First, let's explore the decision boundary using k-NN.\n",
    "\n",
    "Pick two features. I recommend to use a few that show relative strong correlation with diagnosis,\n",
    "and that vary widely across the scatterplot (not just highly correlated narrow line). Feel free to use a combination you already plotted above.\n",
    "\n",
    "1: Predict the diagnosis on a grid (say, 100 × 100, or less if this is too slow) that covers the range of the explanatory variables. Use k-NN with k = 3..7 (pick just one value). This gives you 100x100 predicted diagnoses. See the example code.\n",
    "\n",
    "Note: if your features are of very di􏰅erent scale, you should either scale these into a roughly equal scale, or use a metric that does this with you. Consult James et al. (2015, p 217)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Plot the actual data and the decision boundary on the same plot. Ensure that actual observations and predictions are clearly distinguishable, and that one can easily understand the color code.\n",
    "\n",
    "We just generate some random data and use logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: Describe your observations. How good is kNN in picking up the actual shape? Does it also pick up noise?\n",
    "\n",
    "Note: unless you do cross-validation, you cannot know if the model picks up noise (over􏰀t). Here I just ask your best judgement, not any formal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Logistic Regression\n",
    "\n",
    "Now repeat the process above by logistic regression. Pick the same features as what you used for k-NN above.\n",
    "\n",
    "1: Fit a logistic regression model with these two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Predict the diagnosis on a similar grid. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: . . . and create a similar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4: Describe your observations. How does the result for kNN compare to that for Logistic Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Feature Engineering\n",
    "\n",
    "So far you were using just two of the existing features in the data. However, now let's create a few more.\n",
    "\n",
    "1: Use the two features you used above to compute some new ones. Let's denote your original features\n",
    "by x and x . Examples you may create include: x2, x2, x ·x , 1(x > 5), 1(x < 1)·x2, logx 12 12121121\n",
    "...You can use all sorts of mathematical operations as long as a) you only use x1 and x2 and no other features, and b) the original and the engineered features remain linearly independent (that is, you don't create features like α · x1 + β · x2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Fit a logistic regression model. However, this time pick both x1, x2, and some of your engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: Create the decision boundary plot.\n",
    "\n",
    "Although you now have more than two features, you still plot it using the x1􏰃x2 axes as above. This is because all your engineered features are functionally dependent on x1 and x2 only: if we know values of x1 and x2, we also know exactly the values of your engineered features. We have more than two features but these are not independent (but they should still be linearily independent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4: Comment on the shape of the boundary. What do you think, how well can you capture the actual boundary? What about over􏰀tting? (Again, I ask about your judgement, not about any formal analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5: Repeat the exercise a few times where you pick/engineer di􏰅erent new features, and try to get as reasonable boundary as you can.\n",
    "\n",
    "As above, I am asking 􏰁reasonable􏰂 boundary in the sense of your best judgement. No actual cross validation is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I have mixed experience with scikit-learn.linear_model.LogisticRegression. It occasion- ally appears the default convergence tolerance is far too big, and the default liblinear solver too imprecise. Setting tolerance to 10−12 and solver to lbfgs improved the results for me, but did not make it 􏰆awless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Support Vector Machines\n",
    "\n",
    "\n",
    "As you may have guessed, here your task is to do essentially the same thing but using SVM instead of the other methods. Use the same features what what you used above for k-NN and logistic, but instead of playing with feature engineering, you will play with di􏰅erent kernels.\n",
    "\n",
    "1: Fit a SVM regression model with these two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Predict the diagnosis on a similar grid. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: . . . and create a similar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4: Repeat with a few di􏰅erent kernels (check the software documentation about how to choose kernels and related parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5: Describe your observations. Which kernel gives the most reasonable results? How does the result for kNN compare to that for Logistic Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve: which estimator is the best\n",
    "\n",
    "Finally, let's see which estimator is the best. We compare these in two ways: 􏰀rst using accuracy, precision, recall; and thereafter with ROC curve. We stay with the two features you have been using so far, for the logistic one you also add the engineered features. Consult James et al. (2015, p 148) for what is the ROC curve. Now we are going to compare the best models of k-NN, logistic regression, and SVM.\n",
    "\n",
    "1: Split your data into training-validation parts (say, 75-25)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Select good models from all of your previous sections (i.e. k-NN, logistic, SVM). You may select all models you tried, but if some of those were clearly inferior feel free to skip those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: Train each model on training data. Using the validation data for prediction and present the confusion matrix and compute accuracy, precision, recall, and F-score.\n",
    "\n",
    "Note: in this point you have to implement these algorithms yourself! do not use existing libraries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare these models with ROC curves by playing with di􏰅erent thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4: Use the models you used above for computing the accuracy and all that. Now instead of predicting the oucome class, predict probability that the outcome belongs to a given class (say, 􏰁Malignant􏰂). The library you use may do it already anyway.\n",
    "\n",
    "Note that while Logistic regression and SVM normally predict probability of the target class, at least internally, this may not be the case for k-NN. If the software you are using supports it, use the proportion of the 􏰁successes􏰂 among your k nearest neighbors as the probability (well, it only makes sense if k > 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5: Now pick a sequence of probability thresholds between 0 and 1 (for instance, 0, 0.1, 0.2, ...). For each model and each threshold, treat your predictions to be 􏰁Malignant􏰂 if it it's probability is at least as big as the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6: Based on these predictions, compute false positive rate and true positive rate for each threshold value. Plot these rates for each threshold and each model. This is your ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7: Comment your results. Which model is the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
