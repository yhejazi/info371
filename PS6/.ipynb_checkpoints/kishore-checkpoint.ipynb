{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO371 Problem Set 4: logistic regression, SVM, classi􏰀cation\n",
    "Your name: Kishore Vasan\n",
    "Deadline: Tue, Mar 5 midnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This is the last problem set of this course 􏰇And you will again work with rotten tomatoes. The good news are that you already know this dataset somewhat. However, now we will explore the data a little bit more, and thereafter you will implement your own brand new shiny Naive Bayes to categorize the quotes into rotten/fresh, and 􏰀nd the optimal smoothing parameters with your own brand-even-newer and even-􏰆ashier k-fold cross validation. We also implement the three-fold data split with test data set aside for the 􏰀nal performance measure only.\n",
    "\n",
    "Please submit a) your code (notebooks, rmd, whatever) and b) the results in a 􏰀nal output form (html or pdf). You are free to choose either R or python for solving this problem set.\n",
    "\n",
    "You are welcome to answer some of the questions on paper but please include the result as an image in your 􏰀nal 􏰀le. Note that you can easily include images in both notebooks and .rmd􏰅besides of the code, both are just markdown documents.\n",
    "\n",
    "Working together is fun and useful but you have to submit your own work. Discussing the solutions and problems with your classmates is all right but do not copy-paste their solution! Please list all your collaborators below:\n",
    "\n",
    "1. Yasmine Hejazi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotten Tomatoes\n",
    "Our fi􏰀rst task is to load, clean and explore the Rotten Tomatoes movie reviews data. Please familiarize yourself a little bit with the webpage. Brie􏰆y, approved critics can write reviews for movies, and evaluate the movie as 􏰁fresh􏰂 or 􏰁rotten􏰂. The webpage normally shows a short 􏰁quote􏰂 from each critic, and whether it was evaluated as fresh or rotten. You will work on these quotes below.\n",
    "\n",
    "The central variables in rotten-tomatoes.csv are the following: \n",
    "\n",
    "**critic** name of the critic\n",
    "\n",
    "**fresh** evaluation: 'fresh' or 'rotten'\n",
    "\n",
    "**quote** short version of the review\n",
    "\n",
    "**review_date** when the review was written\n",
    "\n",
    "There are more variables like links to IMDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "Load data and split it into working and testing chunks. But before you begin: ensure you can save a dataframe in a format you can load back in afterwards. pd.to_csv is a good bet, but it has a lot of options which may screw up the way you read data. Ensure you can store data in a way that you can read it back in correctly, including that missings remain missings.\n",
    "\n",
    "1) create a tiny toy data frame that includes some numbers, strings, and missings. Save it and ensure you can reload it in the correct form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Name   Age\n",
      "0  bob  37.0\n",
      "1  jon  23.0\n",
      "2  tom   NaN\n"
     ]
    }
   ],
   "source": [
    "data = [['bob', 37], ['jon', 23], ['tom', ]] \n",
    "df = pd.DataFrame(data, columns = ['Name', 'Age'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(path_or_buf='toy.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Name   Age\n",
      "0  bob  37.0\n",
      "1  jon  23.0\n",
      "2  tom   NaN\n"
     ]
    }
   ],
   "source": [
    "toy = pd.read_csv(\"toy.csv\")\n",
    "print(toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are good to go:\n",
    "\n",
    "2) load the data (available on canvas: 􏰀les/data/rotten-tomatoes.csv). DO NOT LOOK AT IT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of cases:', 13442)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>link</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Derek Adams</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>http://www.timeout.com/film/reviews/87745/toy-...</td>\n",
       "      <td>Time Out</td>\n",
       "      <td>So ingenious in concept, design and execution ...</td>\n",
       "      <td>2009-10-04 00:00:00</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy Story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Richard Corliss</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>http://www.time.com/time/magazine/article/0,91...</td>\n",
       "      <td>TIME Magazine</td>\n",
       "      <td>The year's most inventive comedy.</td>\n",
       "      <td>2008-08-31 00:00:00</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy Story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>David Ansen</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>http://www.newsweek.com/id/104199</td>\n",
       "      <td>Newsweek</td>\n",
       "      <td>A winning animated feature that has something ...</td>\n",
       "      <td>2008-08-18 00:00:00</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy Story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leonard Klady</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>http://www.variety.com/review/VE1117941294.htm...</td>\n",
       "      <td>Variety</td>\n",
       "      <td>The film sports a provocative and appealing st...</td>\n",
       "      <td>2008-06-09 00:00:00</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy Story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jonathan Rosenbaum</td>\n",
       "      <td>fresh</td>\n",
       "      <td>114709</td>\n",
       "      <td>http://onfilm.chicagoreader.com/movies/capsule...</td>\n",
       "      <td>Chicago Reader</td>\n",
       "      <td>An entertaining computer-generated, hyperreali...</td>\n",
       "      <td>2008-03-10 00:00:00</td>\n",
       "      <td>9559</td>\n",
       "      <td>Toy Story</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               critic  fresh    imdb  \\\n",
       "0         Derek Adams  fresh  114709   \n",
       "1     Richard Corliss  fresh  114709   \n",
       "2         David Ansen  fresh  114709   \n",
       "3       Leonard Klady  fresh  114709   \n",
       "4  Jonathan Rosenbaum  fresh  114709   \n",
       "\n",
       "                                                link     publication  \\\n",
       "0  http://www.timeout.com/film/reviews/87745/toy-...        Time Out   \n",
       "1  http://www.time.com/time/magazine/article/0,91...   TIME Magazine   \n",
       "2                  http://www.newsweek.com/id/104199        Newsweek   \n",
       "3  http://www.variety.com/review/VE1117941294.htm...         Variety   \n",
       "4  http://onfilm.chicagoreader.com/movies/capsule...  Chicago Reader   \n",
       "\n",
       "                                               quote          review_date  \\\n",
       "0  So ingenious in concept, design and execution ...  2009-10-04 00:00:00   \n",
       "1                  The year's most inventive comedy.  2008-08-31 00:00:00   \n",
       "2  A winning animated feature that has something ...  2008-08-18 00:00:00   \n",
       "3  The film sports a provocative and appealing st...  2008-06-09 00:00:00   \n",
       "4  An entertaining computer-generated, hyperreali...  2008-03-10 00:00:00   \n",
       "\n",
       "   rtid      title  \n",
       "0  9559  Toy Story  \n",
       "1  9559  Toy Story  \n",
       "2  9559  Toy Story  \n",
       "3  9559  Toy Story  \n",
       "4  9559  Toy Story  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotten = pd.read_csv(\"rotten-tomatoes.csv\")\n",
    "print(\"Number of cases:\", len(rotten))\n",
    "rotten.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) split the dataset into working-testing parts (80/20 or so). Note that sklearn's train_test_split can easily handle dataframes. Just for your con􏰀rmation, ensure that the size of the working and testing data look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(rotten, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Length of train:', 10753)\n",
      "('Length of test:', 2689)\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of train:\", len(train))\n",
    "print(\"Length of test:\", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) now save the test data and delete it from memory. Use python's del statement, or R-s rm function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.to_csv(path_or_buf='test_rotten.csv', index = False)\n",
    "del test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore and clean the data\n",
    "Now when the test data is put aside, we can breath out and take a closer look how does the work data look like.\n",
    "\n",
    "1) Take a look at a few lines of data (you may use pd.sample for this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>fresh</th>\n",
       "      <th>imdb</th>\n",
       "      <th>link</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3372</th>\n",
       "      <td>Jonathan Rosenbaum</td>\n",
       "      <td>fresh</td>\n",
       "      <td>53459</td>\n",
       "      <td>http://onfilm.chicagoreader.com/movies/capsule...</td>\n",
       "      <td>Chicago Reader</td>\n",
       "      <td>As absurd and as beautiful as a fairy tale.</td>\n",
       "      <td>2007-09-26 00:00:00</td>\n",
       "      <td>770698712</td>\n",
       "      <td>Eyes Without a Face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3669</th>\n",
       "      <td>NaN</td>\n",
       "      <td>fresh</td>\n",
       "      <td>46250</td>\n",
       "      <td>http://www.time.com/time/magazine/article/0,91...</td>\n",
       "      <td>TIME Magazine</td>\n",
       "      <td>The newcomer named Audrey Hepburn gives the po...</td>\n",
       "      <td>2009-02-02 00:00:00</td>\n",
       "      <td>18129</td>\n",
       "      <td>Roman Holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11132</th>\n",
       "      <td>Kenneth Turan</td>\n",
       "      <td>fresh</td>\n",
       "      <td>132347</td>\n",
       "      <td>http://www.calendarlive.com/movies/reviews/cl-...</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>An effort you end up admiring more than comple...</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>10623</td>\n",
       "      <td>Mystery Men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13084</th>\n",
       "      <td>Kevin Thomas</td>\n",
       "      <td>rotten</td>\n",
       "      <td>174204</td>\n",
       "      <td>http://www.calendarlive.com/movies/reviews/cl-...</td>\n",
       "      <td>Los Angeles Times</td>\n",
       "      <td>Much effort and expertise have gone into the m...</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>15698</td>\n",
       "      <td>Simpatico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5507</th>\n",
       "      <td>Roger Ebert</td>\n",
       "      <td>fresh</td>\n",
       "      <td>91867</td>\n",
       "      <td>http://www.rogerebert.com/reviews/a-room-with-...</td>\n",
       "      <td>Chicago Sun-Times</td>\n",
       "      <td>It is an intellectual film, but intellectual a...</td>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>17636</td>\n",
       "      <td>A Room With A View</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   critic   fresh    imdb  \\\n",
       "3372   Jonathan Rosenbaum   fresh   53459   \n",
       "3669                  NaN   fresh   46250   \n",
       "11132       Kenneth Turan   fresh  132347   \n",
       "13084        Kevin Thomas  rotten  174204   \n",
       "5507          Roger Ebert   fresh   91867   \n",
       "\n",
       "                                                    link        publication  \\\n",
       "3372   http://onfilm.chicagoreader.com/movies/capsule...     Chicago Reader   \n",
       "3669   http://www.time.com/time/magazine/article/0,91...      TIME Magazine   \n",
       "11132  http://www.calendarlive.com/movies/reviews/cl-...  Los Angeles Times   \n",
       "13084  http://www.calendarlive.com/movies/reviews/cl-...  Los Angeles Times   \n",
       "5507   http://www.rogerebert.com/reviews/a-room-with-...  Chicago Sun-Times   \n",
       "\n",
       "                                                   quote          review_date  \\\n",
       "3372         As absurd and as beautiful as a fairy tale.  2007-09-26 00:00:00   \n",
       "3669   The newcomer named Audrey Hepburn gives the po...  2009-02-02 00:00:00   \n",
       "11132  An effort you end up admiring more than comple...  2000-01-01 00:00:00   \n",
       "13084  Much effort and expertise have gone into the m...  2000-01-01 00:00:00   \n",
       "5507   It is an intellectual film, but intellectual a...  2000-01-01 00:00:00   \n",
       "\n",
       "            rtid                title  \n",
       "3372   770698712  Eyes Without a Face  \n",
       "3669       18129        Roman Holiday  \n",
       "11132      10623          Mystery Men  \n",
       "13084      15698            Simpatico  \n",
       "5507       17636   A Room With A View  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) print out all variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable names:\n",
      "Index([u'critic', u'fresh', u'imdb', u'link', u'publication', u'quote',\n",
      "       u'review_date', u'rtid', u'title'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print 'Variable names:'\n",
    "print train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) create a summary table (maybe more like a bullet list) where you print out the most important summary statistics for the most interesting variables. The most interesting facts you should present should include: a) number of missings for fresh and quote; b) all diff􏰄erent values for fresh/rotten evaluations; c) counts or percentages of these values; d) number of zero-length or only whitespace quote-s; e) minimum-maximum-average length of quotes (either in words, or in characters). (Can you do this as an one-liner?); f) how many reviews are in data multiple times. Feel free to add more fi􏰀gures you consider relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- number of missings for fresh and quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critic         572\n",
      "fresh            0\n",
      "imdb             0\n",
      "link             0\n",
      "publication      0\n",
      "quote            0\n",
      "review_date      0\n",
      "rtid             0\n",
      "title            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "na = train.isna().sum()\n",
    "print na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- all diff􏰄erent values for fresh/rotten evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different evaluation values: \n",
      "['fresh', 'rotten', 'none']\n",
      "Number of unique values: 3\n"
     ]
    }
   ],
   "source": [
    "print 'Different evaluation values: \\n', list(train.fresh.unique())\n",
    "print \"Number of unique values:\", train.fresh.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- counts or percentages of these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentages of evaluation values:\n",
      "fresh     62.140798\n",
      "rotten    37.682507\n",
      "none       0.176695\n",
      "Name: fresh, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print 'Percentages of evaluation values:'\n",
    "print train['fresh'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- number of zero-length or only whitespace quote-s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zero-length quotes: 0\n"
     ]
    }
   ],
   "source": [
    "print('Number of zero-length quotes:', len(train[train.quote.str.len() == 0] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- minimum-maximum-average length of quotes (either in words, or in characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Maximum length of quote: 256\n",
      "Minimum length of quote: 6\n",
      "Average length of quote: 121.0\n"
     ]
    }
   ],
   "source": [
    "quotes = train.quote\n",
    "len_of_quote = [len(i) for i in quotes]\n",
    "print 'Maximum length of quote:', max(len_of_quote)\n",
    "print 'Minimum length of quote:', min(len_of_quote)\n",
    "print 'Average length of quote:', round(sum(len_of_quote)/len(len_of_quote), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- how many reviews are in data multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: 386\n"
     ]
    }
   ],
   "source": [
    "print \"Number of duplicates:\", train.shape[0] - train.drop_duplicates().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feel free to add more fi􏰀gures you consider relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count          mean           std      min    median          max\n",
      "imdb  10753.0  1.559055e+05  1.669296e+05  13442.0  114113.0    1190539.0\n",
      "rtid  10753.0  6.038135e+07  1.878349e+08     11.0   13380.0  771031792.0\n"
     ]
    }
   ],
   "source": [
    "# Get other summary stats\n",
    "summary = train.describe().transpose()\n",
    "summary.columns = ['count','mean','std','min','25%','median','75%','max']\n",
    "summary = summary[['count','mean','std','min','median','max']]\n",
    "print summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Now when you have an overview what you have in data, clean it by removing all the inconsistencies the table reveals. We have to ensure that the central variables: quote and fresh are not missing, and quote is not an empty string (or just contain spaces and such).\n",
    "\n",
    "I strongly recommend to do it as a standalone function because at the end you have to perform exactly the same cleaning operations with your test data too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    # remove 'none' for fresh\n",
    "    df = df[df.fresh != 'none']\n",
    "    # remove missing quotes?\n",
    "    df = df[df.quote.str.len() > 0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_clean = clean(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fresh     6682\n",
       "rotten    4052\n",
       "Name: fresh, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean.fresh.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Naïve Bayes\n",
    "Now where you are familiar with the data, it's time to get serious and implement the Naive Bayes classi􏰀er from scratch. But fi􏰀rst things fi􏰀rst.\n",
    "\n",
    "1) Ensure you are familiar with Naive Bayes. Consult the readings, available on canvas. Schutt & O'Neill is an easy and accessible (and long) introduction, Whitten & Frank is a lot shorter but still accessible introduction. [Completed]\n",
    "\n",
    "2) Convert your data (quotes) into bag-of-words. Your code should look something along the lines as in PS4. However, now we don't want BOW that contains counts of words in quotes, but just 1/0 (or true/- false) for the presence/non-presence of the words. Convert the count-based BOW into such a presence BOW. Hint: think in terms of vectorized (universal) functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]]), '\\n')\n",
      "('Number of rows:', 10734)\n"
     ]
    }
   ],
   "source": [
    "# initialize the vectorizer\n",
    "vectorizer = CountVectorizer(min_df=0)\n",
    "\n",
    "# create the dictionary\n",
    "vectorizer.fit(train_clean.quote)\n",
    "\n",
    "# `fit` builds the vocabulary\n",
    "# transform your data into the BOW array\n",
    "X = vectorizer.transform(train_clean.quote).toarray()\n",
    "words_list = list(vectorizer.get_feature_names())\n",
    "\n",
    "print(X, '\\n')\n",
    "print('Number of rows:', len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of BOW: (10734, 18994)\n",
      "check this works:  1\n"
     ]
    }
   ],
   "source": [
    "# TODO: Convert BOW to boolean 1/0\n",
    "X[X>0] = 1\n",
    "print \"shape of BOW:\", X.shape\n",
    "print \"check this works: \", X.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Split your work data and target (i.e. the variable fresh) into training and validation chunks (80/20 or so). Later we also do cross-validation, but for now, a simple training/validation will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vtrain, val = train_test_split(train_clean, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Now you are ready with the preparatory work and it's time to dive into the **real** thing. Let's implement Naive Bayes. Use only training data in the fi􏰀tting below.\n",
    "\n",
    "4) Compute the unconditional (log) probability that the tomato is fresh/rotten, log Pr(F), and log Pr(R). These probabilities are based on the values of fresh variable but not on the words the quotes contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probablity of Fresh: -0.473339804141\n",
      "Log probability of Rotten: -0.975293576435\n"
     ]
    }
   ],
   "source": [
    "pr_fr = vtrain.fresh.value_counts(normalize=True)\n",
    "pr_f, pr_r = numpy.log(pr_fr)\n",
    "print \"Log probablity of Fresh:\", pr_f\n",
    "print \"Log probability of Rotten:\", pr_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) For each word w, compute log Pr(w|F) and log Pr(w|R), the (log) probability that the word is present in a fresh/rotten review. These probabilities can easily be calculated from counts of how many times these words are present for each class.\n",
    "\n",
    "Hint: these computations are based on your BOW-s X. Look at ways to sum along columns in this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw probability----\n",
      "pr(w|f):\n",
      "[0.25       1.         0.71428571 ... 0.         0.66666667 0.        ]\n",
      "pr(w|r)\n",
      "[0.75       0.         0.28571429 ... 1.         0.33333333 1.        ]\n",
      "log probabilities---\n",
      "log pr(w|f):\n",
      "[-1.38629436  0.         -0.33647224 ...        -inf -0.40546511\n",
      "        -inf]\n",
      "log pr(w|r)\n",
      "[-0.28768207        -inf -1.25276297 ...  0.         -1.09861229\n",
      "  0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kishorevasan/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:26: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/kishorevasan/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:27: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "#get the indices that contain fresh or rotten\n",
    "boolean_fresh = [1 if i == 'fresh' else 0 for i in train_clean.fresh]\n",
    "fresh_indices = []\n",
    "rotten_indices = []\n",
    "\n",
    "for i, j in enumerate(boolean_fresh):\n",
    "    if j == 1:\n",
    "        fresh_indices.append(i)\n",
    "    else:\n",
    "        rotten_indices.append(i)\n",
    "        \n",
    "#probability of w|fresh\n",
    "pr_w_f = numpy.divide(X[numpy.array(fresh_indices),:].sum(axis = 0).astype(float),X.sum(axis = 0).astype(float))\n",
    "\n",
    "#probability of w|rotten\n",
    "pr_w_r = numpy.divide(X[numpy.array(rotten_indices),:].sum(axis = 0).astype(float),X.sum(axis = 0).astype(float))\n",
    "\n",
    "print \"raw probability----\"\n",
    "print \"pr(w|f):\"\n",
    "print pr_w_f\n",
    "print \"pr(w|r)\"\n",
    "print pr_w_r\n",
    "\n",
    "\n",
    "#take the log of probability\n",
    "pr_w_f = numpy.log(pr_w_f)\n",
    "pr_w_r = numpy.log(pr_w_r)\n",
    "print \"log probabilities---\"\n",
    "print \"log pr(w|f):\"\n",
    "print pr_w_f\n",
    "print \"log pr(w|r)\"\n",
    "print pr_w_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are done with the estimator. Your 􏰀tted model is completely described by these four probability vectors: log Pr(F), log Pr(R), log Pr(w|F), log Pr(w|R). Let's now turn to prediction, and pull out your validation data (not the test data!).\n",
    "\n",
    "6) For both destination classes, F and R, compute the log-likelihood that the quote belongs to this class. I think we mentioned log-likelihood in the class, it is what is given inside the brackets in equation (1) on slide 28, and the equations on Schutt 􏰁Doing Data Science􏰂, page 102. On the slides we have the log-likelihood essentially as (although we do not write it out):\n",
    "\n",
    "􏰈\n",
    "li(c) = log Pr(c) + where c ∈ {F, R} is the class, i is the review, j indexes words, and wij is the j-th word of the review\n",
    "i.\n",
    "\n",
    "Computing these likelihoods involves sums of the previously computed probabilities, log Pr(w|F), and BOW elements xij. Check out np.apply_along_axis (or R's apply) that can be used to apply a function on matrix columns/rows so you can create a fairly good one-liner to compute log-likelihood. Loops are 􏰀ne too, just slower and less compact.\n",
    "\n",
    "\n",
    "Based on the log-likelihoods, predict the class F or R for each quote in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(quote):\n",
    "    words = quote.split(' ')\n",
    "    word_idx = []\n",
    "    for i in words:\n",
    "        if i in words_list:\n",
    "            word_idx.append(words_list.index(i))\n",
    "    tmp_p_f = pr_f\n",
    "    tmp_p_r = pr_r\n",
    "\n",
    "    if len(word_idx)>0:\n",
    "        tmp_p_f += numpy.sum(pr_w_f[numpy.array(word_idx)])\n",
    "        tmp_p_r += pr_r + numpy.sum(pr_w_r[numpy.array(word_idx)])\n",
    "    \n",
    "    if tmp_p_f > tmp_p_r:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "#use v train to predict\n",
    "predicted = vtrain.quote.apply(predict).to_frame().quote\n",
    "predicted = list(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Print the resulting confusion matrix and accuracy (feel free to use existing libraries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 82.042622569\n",
      "Confusion Matrix:\n",
      "[[1696 1542]\n",
      " [   0 5349]]\n"
     ]
    }
   ],
   "source": [
    "# finding accuracy\n",
    "y = [1 if i == 'fresh' else 0 for i in clean_vtrain.fresh]\n",
    "output = [a == p for a,p in zip(predicted,y)]\n",
    "accuracy = float(numpy.sum(output))/ float(len(output)) * 100\n",
    "mat = confusion_matrix(y, predicted)\n",
    "print \"Accuracy of the model:\", accuracy\n",
    "print \"Confusion Matrix:\"\n",
    "print mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interpretation\n",
    "Now it is time to look at your 􏰀fitted model a little bit closer. NB model probabilities are rather easy to understand and interpret. The task here is to 􏰀nd the best words to predict a fresh, and a rotten review. And we only want to look at words that are reasonably frequent, say more frequent than 30 times in the data.\n",
    "\n",
    "1) Extract from your conditional probability vectors log Pr(F) and log Pr(R) the probabilities that cor- respond to frequent words only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of top fresh words:\n",
      "[-0.52902136 -0.34294475 -0.46826601 -0.4735687  -0.39890771 -0.54320703\n",
      " -0.40546511 -0.39465419 -0.34092659 -0.54821316 -0.38484582 -0.22957444\n",
      " -0.39086631 -0.50854512 -0.28090239 -0.51295102 -0.34662461 -0.26072626\n",
      " -0.51082562 -0.24946086 -0.23483959 -0.22314355 -0.37806613 -0.42228538\n",
      " -0.41230194 -0.24231347 -0.31015493 -0.60025343 -0.57191332 -0.59911823\n",
      " -0.7039581  -0.36290549 -0.58314629 -0.70621926 -0.23712979 -0.49324072\n",
      " -0.52849984 -0.59394053 -0.39768297 -0.51581317 -0.37297965 -1.13707857\n",
      " -0.59241436 -0.131336   -0.11506933 -0.15684247 -0.40869613 -0.51930025\n",
      " -0.55681074 -0.53657812 -0.68491668 -0.20334092 -0.53817108 -0.46112596\n",
      " -0.56346936 -0.47957308 -0.57380042 -0.33135714 -0.94803943 -0.33270575\n",
      " -0.22778393 -0.18859117 -0.5602011  -0.4927868  -0.28090239 -0.55137172\n",
      " -0.21130909 -0.30538165 -0.29924289 -0.50106945 -0.54715198 -0.21511138\n",
      " -0.26072626 -0.27820333 -0.17284281 -0.55961579 -0.29423947 -0.24027983\n",
      " -0.28768207 -0.46262352 -0.40546511 -0.52155623 -0.52218938 -0.51563333\n",
      " -0.20479441 -0.05556985 -0.5581109  -0.28768207 -0.2534489  -0.26072626\n",
      " -0.33647224 -0.37085958 -0.29546421 -0.30421137 -0.37647757 -0.28030197\n",
      "  0.         -0.24294618 -0.73288751 -0.72105597 -0.63252256 -0.40032371\n",
      " -0.45581899 -0.47855069 -0.42199441 -0.57763429 -0.39641527 -0.80389854\n",
      " -0.70006762 -0.3254224  -0.56502608 -0.36290549 -0.51082562 -0.42381425\n",
      " -0.39147887 -0.15634607 -0.54719327 -0.62645581 -0.57536414 -0.26528114\n",
      " -0.32721291 -0.64375443 -0.5798185  -0.22314355 -0.23483959 -0.15790303\n",
      " -0.69314718 -0.15145162 -0.15082289 -0.24740817 -0.45425527 -0.57873683\n",
      " -0.26484258 -0.58900692 -0.37267529 -0.5075946  -0.10265415 -0.189242\n",
      " -0.1847341  -0.06252036 -0.28030197 -0.40546511 -0.26826399 -0.25671985\n",
      " -0.36876374 -0.5389965  -0.27996003 -0.72054615 -0.29479954 -0.18571715\n",
      " -0.28768207 -0.34754246 -0.66457381 -0.45198512 -0.49247649 -0.31015493\n",
      " -0.37543654 -0.37647757 -0.26495382 -0.52702031 -0.24740817 -0.3765761\n",
      " -0.50575866 -0.22778393 -0.27443685 -0.51573964 -0.33293866 -0.28295393\n",
      " -0.05556985 -0.39027494 -0.59136449 -0.38566248 -0.70621926 -0.44285264\n",
      " -0.61090908 -0.10227885 -0.42488319 -0.67990195 -0.47489361 -0.40332607\n",
      " -0.45473616 -0.25131443 -0.21494678 -0.09097178 -0.597837   -0.49549126\n",
      " -0.41839753 -0.62371867 -0.53630471 -0.3384544  -0.43379561 -0.63548154\n",
      " -0.34687094 -0.2853208  -0.15082289 -0.41197979 -0.48550782 -0.36537065\n",
      " -0.27625338 -0.30368241 -0.20679441 -0.35767444 -0.38882579 -0.44245368\n",
      " -0.40967564 -0.4326161  -0.55061387 -0.13062018 -0.47484105 -0.23361485\n",
      " -0.09097178 -0.50209194 -0.45770951 -0.86776369 -0.46795445 -0.46052489\n",
      " -0.52969411 -0.29924289 -0.55961579 -0.58162591 -0.56798404 -0.19290367\n",
      " -0.47260441 -0.35430994 -0.52763274 -0.08455739 -0.30338098 -0.37948962\n",
      " -0.77110872 -0.64185389 -0.36974703 -0.71667768 -0.56309405 -0.31339043\n",
      " -0.23841102 -0.65252421 -0.56798404 -0.71294981 -0.54159728 -0.35139789\n",
      " -0.49643689 -0.64355024 -0.44333538 -0.7683706  -0.55961579 -0.57443087\n",
      " -0.32059889 -0.64662716 -0.36965566 -0.41551544 -0.54749443 -0.34734191\n",
      " -0.43157625 -0.41185491 -0.3973018  -0.57763429 -0.03571808 -0.58604905\n",
      " -0.42697131 -0.58314629 -0.48550782 -0.36396538 -0.87681192 -0.40546511\n",
      " -0.94160854 -0.16251893 -0.53454215 -0.29546421 -0.60431597 -0.27059771\n",
      " -0.36150198 -0.52438515 -0.34830669 -0.11034806 -0.40546511 -0.74591722\n",
      " -0.37320425 -0.29298712 -0.38193461 -0.48379695 -0.43363599 -0.5476396\n",
      " -0.70818506 -0.59003638 -0.39699048 -0.70865137 -0.68406996 -0.50402288\n",
      " -0.87082836 -0.43891304 -0.56798404 -0.27193372 -0.43152713 -0.5994147\n",
      " -0.28768207 -0.48788405 -0.42708757 -0.48173524 -0.45332113 -0.39873108\n",
      " -0.68974002 -0.70536729 -0.47398505 -0.5070449  -0.27484493 -0.61976463\n",
      " -0.48613301 -0.49765516 -0.33024169 -0.41185491 -0.12516314 -0.18464985\n",
      " -0.23011222 -0.45425527 -0.19105524 -0.34484049 -0.43470193 -0.37806613\n",
      " -0.24027983 -0.52129692 -0.49717353 -0.40546511 -0.66237552 -0.14310084\n",
      " -0.76146642 -0.79078565 -0.21130909 -0.16251893 -0.21414799 -0.11778304\n",
      " -0.62718921 -0.556288   -0.66574821 -0.20359896 -0.26469255 -0.51621647\n",
      " -0.54501699 -0.09844007 -0.61903921 -0.56614749 -0.52515545 -1.09861229\n",
      " -0.18232156 -0.27369583 -0.5389965  -0.17327172 -0.31177962 -0.30110509\n",
      " -0.30010459 -0.42348361 -0.47420826 -0.80562516 -0.47849024 -0.28256697\n",
      " -0.58192155 -0.62860866 -0.49717353 -0.28030197 -0.34550164 -0.76546784\n",
      " -0.74134928 -0.39819235 -0.82773733 -0.79740819 -0.38677298 -0.72330022\n",
      " -0.36842384 -0.8347977  -0.51082562 -0.39616272 -0.35020243 -0.50310358\n",
      " -0.66189464 -0.46262352 -0.37320425 -0.19671029 -0.55594606 -0.24214005\n",
      " -0.35020243 -0.26826399 -0.16332506 -0.5692777  -0.2006707  -0.52752747\n",
      " -0.52827254 -0.37578934 -0.61903921 -0.29479954 -0.62645581 -0.22957444\n",
      " -0.15415068 -0.35667494 -0.40546511 -0.23859246 -0.44747452 -0.28768207\n",
      " -0.19105524 -0.50077529 -0.34887763 -0.18232156 -0.09237332 -0.60143875\n",
      " -0.26236426 -0.19671029 -0.52925933 -0.20409536 -0.40947313 -0.34377154\n",
      " -0.27494305 -0.30228087 -0.23361485 -0.55014395 -0.45859017 -0.47065338\n",
      " -0.48464314 -0.61421992 -0.56211892 -0.64222709 -0.58150721 -0.71606144\n",
      " -0.59339678 -0.42433359 -0.66904963 -0.51512406 -0.30368241 -0.51082562\n",
      " -0.57206925 -0.54654371 -0.41721612 -0.50424665 -0.43058209 -0.49365782\n",
      " -0.53630471 -0.55622882 -0.11122564 -0.61903921 -0.72300014 -0.95551145\n",
      " -0.2570451  -0.35020243 -0.05406722 -0.24362208 -0.55961579 -0.49561621\n",
      " -0.51758241 -0.49247649 -0.62474445 -0.3598546  -0.43363599 -0.40546511\n",
      " -0.48362988 -0.43078292 -0.52005443 -0.60077386 -0.359374   -0.23511974\n",
      " -0.83624802 -0.21706451 -0.55961579 -0.50262886 -0.51082562 -0.44757659\n",
      " -0.33647224 -0.4675171  -0.33400614 -0.73396918 -0.50806319 -0.51957841\n",
      " -0.58604905 -0.52481187 -0.49112055 -0.55961579 -0.44282089 -0.63324904\n",
      " -0.43721381 -0.16705408 -0.45198512 -0.34830669 -0.38851059 -0.48485014\n",
      " -0.06062462 -0.51669074 -0.38776553 -0.18610228 -0.34574587 -0.2135741\n",
      " -0.23450731 -0.38711597 -0.77493661 -0.47849024 -0.58778666 -0.36593427\n",
      " -0.19782574 -0.35340875 -0.50043475 -0.30918828 -0.4974026 ]\n",
      "Probability of top rotten words:\n",
      "[-0.88960325 -0.98373201 -0.97491558 -0.86960362 -0.86270649 -0.91972127\n",
      " -0.91311108 -1.47181653 -0.91629073 -1.56397554 -1.06579744 -1.08507727\n",
      " -0.79556219 -0.83113292 -0.79694397 -0.68245189 -0.81676114 -0.68024378\n",
      " -0.94326185 -0.89035167 -0.80329076 -1.16693153 -0.38677298 -0.80517556\n",
      " -1.0921814  -0.90371195 -0.4608152  -0.85105021 -0.87886433 -0.70144598\n",
      " -1.69281952 -0.87662548 -0.99580279 -0.9650809  -0.82869267 -0.49020634\n",
      " -1.26224171 -0.84651798 -0.94397416 -0.85839675 -1.33500107 -0.93110582\n",
      " -0.86416167 -1.09861229 -0.90040769 -0.89948361 -0.90912224 -0.84930791\n",
      " -0.65492597 -0.66599619 -1.10897508 -1.0049268  -0.96674451 -1.06635143\n",
      " -0.82376736 -1.11696143 -0.59344782 -0.6862743  -0.84012937 -1.18958407\n",
      " -0.15415068 -0.86410498 -0.4139758  -0.62645581 -0.74510692 -0.82098055\n",
      " -0.69314718 -0.82235891 -1.4581201  -0.80940699 -0.92115692 -0.26662866\n",
      " -1.17632127 -0.66647893 -1.22561198 -0.72256107 -0.3471962  -0.94446161\n",
      " -1.16151795 -1.45775333 -0.89248008 -1.15902142 -0.31845373 -0.92393965\n",
      " -0.90896469 -1.26165192 -1.40061441 -1.12970288 -1.13943428 -0.68024378\n",
      " -1.02778624 -0.78275934 -1.06087196 -0.7065702  -0.97273204 -1.10290414\n",
      " -1.64291384 -0.7985077  -0.47000363 -0.93974239 -1.07323862 -0.76775804\n",
      " -0.87924946 -1.04425108 -0.75434274 -1.39341183 -1.18397214 -1.20164451\n",
      " -1.1327453  -0.52735493 -1.02850272 -1.09024404 -1.04642654 -0.85942787\n",
      " -0.97281853 -0.51725651 -0.51082562 -0.92953596 -1.00166142 -0.54453668\n",
      " -0.98425391 -0.99682959 -0.8886392  -0.84729786 -0.44531102 -0.81868493\n",
      " -0.83624802 -1.20951298 -0.89159812 -0.38566248 -0.62082652 -0.7472144\n",
      " -0.67015766 -0.84267891 -1.31291182 -0.73549051 -0.67372909 -0.74533293\n",
      " -1.02692036 -0.62318859 -0.82787977 -1.29358496 -1.17432411 -0.86369167\n",
      " -1.22609472 -1.04835045 -1.08595389 -0.82376736 -0.8131065  -1.05693959\n",
      " -0.81676114 -0.27958486 -0.53803819 -0.49429632 -0.88173835 -0.79064555\n",
      " -1.43937178 -0.89629007 -1.22377543 -1.09861229 -0.64302279 -0.86349255\n",
      " -0.67833209 -0.80812517 -1.11578009 -0.67787971 -0.70230755 -0.92658242\n",
      " -0.54232429 -0.83624802 -0.49783843 -1.04844148 -0.79658277 -0.95172117\n",
      " -1.05672135 -0.96157732 -1.00926719 -1.11221794 -0.69656599 -0.6810746\n",
      " -0.97422857 -0.92198875 -1.4258252  -0.77234422 -0.95451194 -0.93637749\n",
      " -1.08595389 -1.58203894 -1.04258446 -0.93712482 -0.72489588 -0.62919846\n",
      " -0.60419969 -0.37267529 -0.76376475 -0.85175221 -0.21772348 -0.72131806\n",
      " -0.86710049 -0.77318989 -0.83865476 -0.89517381 -0.40546511 -0.97386058\n",
      " -0.59205106 -0.93712482 -1.23053983 -0.6257059  -0.64716207 -1.11331844\n",
      " -0.57454144 -0.5987375  -1.13707857 -0.6638768  -1.1770839  -0.56909453\n",
      " -1.11748077 -0.92798677 -0.72540804 -0.61903921 -0.83455883 -0.89174962\n",
      " -0.89067816 -0.76460614 -1.2039728  -1.54992378 -1.01954377 -1.22240651\n",
      " -0.7941231  -1.09064412 -1.23474446 -0.86006817 -1.00014603 -0.97974727\n",
      " -0.95689649 -0.7788419  -0.84397007 -0.74679989 -0.8188354  -0.67074624\n",
      " -0.80396155 -1.06191092 -0.71783979 -0.90987752 -0.91629073 -0.83093088\n",
      " -0.86499744 -1.07551657 -0.92624106 -1.05019519 -0.85183173 -0.77318989\n",
      " -0.66415964 -0.48550782 -0.93954759 -0.40546511 -0.9062404  -0.02666825\n",
      " -0.76657465 -1.1965927  -0.9585235  -0.90260528 -0.56798404 -0.84729786\n",
      " -0.92871325 -0.91629073 -1.01936292 -0.98498726 -1.25895494 -0.65392647\n",
      " -0.92044874 -0.90330354 -0.8131065  -0.89567144 -0.94659608 -1.02784322\n",
      " -0.75686299 -1.03798767 -0.6061358  -1.01160091 -1.13340884 -0.95656463\n",
      " -1.22994829 -1.56523182 -0.25131443 -0.61754444 -1.18269541 -1.21163568\n",
      " -0.93208234 -0.93676926]\n"
     ]
    }
   ],
   "source": [
    "# words that appear more than 30 times\n",
    "fresh_X = X[numpy.array(fresh_indices),:]\n",
    "fresh_freq_idx = [i for (i,j) in enumerate(list(fresh_X.sum(axis = 0) >30)) if j == True]\n",
    "\n",
    "# words that appear more than 30 times\n",
    "rotten_X = X[numpy.array(rotten_indices),:]\n",
    "rotten_freq_idx = [i for (i,j) in enumerate(list(rotten_X.sum(axis = 0) >30)) if j == True]\n",
    "\n",
    "#log probability of top words\n",
    "top_p_f = pr_w_f[numpy.array(fresh_freq_idx)]\n",
    "top_p_r = pr_w_r[numpy.array(rotten_freq_idx)]\n",
    "print \"Probability of top fresh words:\"\n",
    "print top_p_f\n",
    "print \"Probability of top rotten words:\"\n",
    "print top_p_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Find 10 best words to predict F and 10 best words to predict R. Hint: imagine we have a review that contains just a single word. Which word will give the highest weight to the probability the review is fresh? Which one to the likelihood it is rotten?\n",
    "\n",
    "Comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top fresh words:\n",
      "[u'celebrates', u'muhammad', u'mulch', u'mulishness', u'mullan', u'mullen', u'muller', u'mulligan', u'muggings', u'languorous']\n",
      "Top rotten words:\n",
      "[u'kooky', u'knockoffs', u'standoff', u'knotting', u'standbys', u'standby', u'knuckleheaded', u'standardized', u'knot', u'zzzzzzzzz']\n"
     ]
    }
   ],
   "source": [
    "top_f_idx = numpy.argsort(pr_w_f)[-10:]\n",
    "top_r_idx = numpy.argsort(pr_w_r)[-10:]\n",
    "\n",
    "#top words\n",
    "top_f_words = [words_list[i] for i in top_f_idx]\n",
    "top_r_words = [words_list[i] for i in top_r_idx]\n",
    "\n",
    "print \"Top fresh words:\"\n",
    "print top_f_words\n",
    "print \"Top rotten words:\"\n",
    "print top_r_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Print out a few missclassi􏰀ed quotes. Can you understand why these are misclassi􏰀ed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "'mulishness', 'mullen' seem to be misclassified. \n",
    "\n",
    "This is probably becuase the words **only** appear in one of the classes. Could be because of **data quality issues or edge cases.** This creates a 100% chance to be in one class and 0% probability to be part of the other class. Additionaly, we didn't do any form of data cleaning like **removing stop words etc**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. NB with smoothing\n",
    "So, now you have your brand-new NB algorithm up and running. As a next step, we add smoothing to it. As you will be doing cross-validation below, your 􏰀rst task is to mold what you did above into two funcions: one for 􏰀tting and another one for predicting.\n",
    "\n",
    "1) Create two functions: one for 􏰀tting NB model, and another to predict outcome based on the fi􏰀tted model.\n",
    "\n",
    "As mentioned above, the model is fully described with 4 probabilities, so your 􏰀tting function may return such a list as the model; and the prediction function may take it as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit NB for given train quotes and y train\n",
    "def fitNB(X_train, y_train, alpha):\n",
    "    global l_p_f, l_p_r, words_list\n",
    "    # initialize the vectorizer\n",
    "    vectorizer = CountVectorizer(min_df=0)\n",
    "\n",
    "    # create the dictionary\n",
    "    vectorizer.fit(X_train)\n",
    "\n",
    "    # `fit` builds the vocabulary\n",
    "    # transform your data into the BOW array\n",
    "    X = vectorizer.transform(X_train).toarray()\n",
    "    words_list = list(vectorizer.get_feature_names())\n",
    "\n",
    "    n_fresh = numpy.sum(y_train == 'fresh') + alpha\n",
    "    n_rotten = numpy.sum(y_train == 'rotten') + alpha\n",
    "    n_total = n_fresh + n_rotten\n",
    "    p_fresh = float(n_fresh) / float(n_total)\n",
    "    p_rotten = float(n_rotten) / float(n_total)\n",
    "    f_counts = X[y_train == 'fresh'].sum(axis = 0) + alpha\n",
    "    r_counts = X[y_train == 'rotten'].sum(axis = 0) + alpha\n",
    "    l_p_f = numpy.log(f_counts / n_fresh)\n",
    "    l_p_r = numpy.log(r_counts / n_rotten)\n",
    "    return l_p_f, l_p_r\n",
    "\n",
    "# create BOW for the quotes and predict\n",
    "def predictNB(quote):\n",
    "    words = quote.split(' ')\n",
    "    word_idx = []\n",
    "    for i in words:\n",
    "        if i in words_list:\n",
    "            word_idx.append(words_list.index(i))\n",
    "    tmp_p_f = p_fresh\n",
    "    tmp_p_r = p_rotten\n",
    "    \n",
    "    if len(word_idx)>0:\n",
    "        tmp_p_f += numpy.sum(l_p_f[numpy.array(word_idx)])\n",
    "        tmp_p_r += pr_r + numpy.sum(l_p_r[numpy.array(word_idx)])\n",
    "    \n",
    "    # if prob of fresh is greater than prob of rotten\n",
    "    if tmp_p_f > tmp_p_r:\n",
    "        return 1\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Add smoothing to the model. See Schutt p 103 and 109. Smoothing amounts to assuming that we have 􏰁seen􏰂 every possible work α 􏰉 0 times already, for both classes. (If you wish, you can also assume you have seen the words α times for F and β times for R). Note that α does not have to be an integer, and typically the best α < 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**We add smoothing to the function itself when calculating the probabilities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Now 􏰀fit a few models with di􏰄erent α-s and see if the accuracy improves compared to the baseline case above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the vectorizer\n",
    "vectorizer = CountVectorizer(min_df=0)\n",
    "\n",
    "# create the dictionary\n",
    "vectorizer.fit(train_clean.quote)\n",
    "\n",
    "# `fit` builds the vocabulary\n",
    "# transform your data into the BOW array\n",
    "X = vectorizer.transform(vtrain.quote).toarray()\n",
    "words_list = list(vectorizer.get_feature_names())\n",
    "\n",
    "print(X, '\\n')\n",
    "print('Number of rows:', len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kishorevasan/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/kishorevasan/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:22: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.0\n",
      "Accuracy of the model: 65.9990684676\n",
      "Alpha: 0.1\n",
      "Accuracy of the model: 69.8649278062\n",
      "Alpha: 0.2\n",
      "Accuracy of the model: 70.4238472287\n",
      "Alpha: 0.3\n",
      "Accuracy of the model: 70.8896134141\n",
      "Alpha: 0.4\n",
      "Accuracy of the model: 71.0759198882\n",
      "Alpha: 0.5\n",
      "Accuracy of the model: 71.2622263624\n",
      "Alpha: 0.6\n",
      "Accuracy of the model: 71.2622263624\n",
      "Alpha: 0.7\n",
      "Accuracy of the model: 71.3553795994\n",
      "Alpha: 0.8\n",
      "Accuracy of the model: 71.3553795994\n",
      "Alpha: 0.9\n",
      "Accuracy of the model: 71.1224965068\n",
      "Alpha: 1\n",
      "Accuracy of the model: 38.3791336749\n"
     ]
    }
   ],
   "source": [
    "X_quote = vtrain.quote\n",
    "y = vtrain.fresh\n",
    "\n",
    "y_output = val.fresh\n",
    "y_output = [1 if i == 'fresh' else 0 for i in y_output]\n",
    "\n",
    "for al in [0.00, 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]:\n",
    "    fitNB(X_quote, y, al)\n",
    "    predicted = val.quote.apply(predictNB).to_frame().quote\n",
    "    predicted = list(predicted)\n",
    "    # finding accuracy\n",
    "    output = [a == p for a,p in zip(predicted,y_output)]\n",
    "    accuracy = float(numpy.sum(output))/ float(len(output)) * 100\n",
    "    print \"Alpha:\", al\n",
    "    print \"Accuracy of the model:\", accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Validation\n",
    "Finally (well, almost 􏰀finally), we do cross-validation. This is another piece of code you have to implement yourself, not use existing libraries.\n",
    "\n",
    "- Implement k-fold CV. I recommend to implement it as a function that a) puts your data into random order; b) splits these into k chunks; c) selects a chunk for testing and the others for training; d) trains your NB model on the training chunks; e) computes accuracy on validating chunk; f) returns mean accuracy over all these k trials. The function should also take α as an argument, this is the hyperparameter you are going to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data_frame(df, num_chunks): \n",
    "    listOfDf = list()\n",
    "    chunk_size = len(df) // num_chunks\n",
    "    for i in range(num_chunks):\n",
    "        listOfDf.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "    return listOfDf\n",
    "\n",
    "def cross_val(data, k, alpha):\n",
    "    print \"num_chunks:\", k\n",
    "    print \"alpha:\", alpha\n",
    "    chunks = split_data_frame(data, k)\n",
    "    accuracy_list = []\n",
    "    for chunk_df in chunks:\n",
    "        tmp_train = data[~data.isin(chunk_df)]\n",
    "        X_quote = tmp_train.quote.astype(str)\n",
    "        y = tmp_train.fresh\n",
    "\n",
    "        y_output = chunk_df.fresh\n",
    "        y_output = [1 if i == 'fresh' else 0 for i in y_output]\n",
    "\n",
    "        fitNB(X_quote, y, alpha)\n",
    "        predicted = chunk_df.quote.apply(predictNB).to_frame().quote\n",
    "        predicted = list(predicted)\n",
    "\n",
    "        # finding accuracy\n",
    "        output = [a == p for a,p in zip(predicted,y_output)]\n",
    "        accuracy = float(numpy.sum(output))/ float(len(output)) * 100\n",
    "        accuracy_list.append(accuracy)\n",
    "       \n",
    "        print \"Accuracy of the model:\", accuracy\n",
    "    print \"Average accuracy:\", numpy.mean(accuracy_list)\n",
    "    print \"------\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_chunks: 5\n",
      "alpha: 0.5\n",
      "Accuracy of the model: 74.091332712\n",
      "Accuracy of the model: 74.4641192917\n",
      "Accuracy of the model: 72.4137931034\n",
      "Accuracy of the model: 72.8797763281\n",
      "Accuracy of the model: 71.6216216216\n",
      "Average accuracy: 73.09412861136998\n"
     ]
    }
   ],
   "source": [
    "# test if it works\n",
    "cross_val(train_clean, 5, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the optimal α by 5-fold CV using your own CV code. You have to **fi􏰀nd** the cross-validated accuracies for a number of α-s between 0 and 1. Present the accuracy as a function of α on a plot and indicate which one is the best α."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_chunks: 5\n",
      "alpha: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kishorevasan/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/kishorevasan/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:22: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 69.7110904007\n",
      "Accuracy of the model: 68.6393289842\n",
      "Accuracy of the model: 66.9617893756\n",
      "Accuracy of the model: 68.4529356943\n",
      "Accuracy of the model: 66.9617893756\n",
      "Average accuracy: 68.14538676607643\n",
      "------\n",
      "num_chunks: 5\n",
      "alpha: 0.1\n",
      "Accuracy of the model: 72.6467847158\n",
      "Accuracy of the model: 73.578751165\n",
      "Accuracy of the model: 71.0158434296\n",
      "Accuracy of the model: 71.9478098788\n",
      "Accuracy of the model: 70.7362534949\n",
      "Average accuracy: 71.98508853681267\n",
      "------\n",
      "num_chunks: 5\n",
      "alpha: 0.4\n",
      "Accuracy of the model: 73.9049394222\n",
      "Accuracy of the model: 74.4175209692\n",
      "Accuracy of the model: 72.0410065238\n",
      "Accuracy of the model: 72.6933830382\n",
      "Accuracy of the model: 71.3886300093\n",
      "Average accuracy: 72.88909599254427\n",
      "------\n",
      "num_chunks: 5\n",
      "alpha: 0.8\n",
      "Accuracy of the model: 74.091332712\n",
      "Accuracy of the model: 74.3709226468\n",
      "Accuracy of the model: 72.4137931034\n",
      "Accuracy of the model: 73.578751165\n",
      "Accuracy of the model: 71.6682199441\n",
      "Average accuracy: 73.22460391425909\n",
      "------\n",
      "num_chunks: 5\n",
      "alpha: 1\n",
      "Accuracy of the model: 37.4184529357\n",
      "Accuracy of the model: 37.3718546132\n",
      "Accuracy of the model: 39.1891891892\n",
      "Accuracy of the model: 38.7232059646\n",
      "Accuracy of the model: 38.5368126747\n",
      "Average accuracy: 38.24790307548928\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "for al in [0.00, 0.1, 0.4, 0.8, 1]:\n",
    "    cross_val(train_clean, 5, al)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final model performance\n",
    "Finally (and now I mean 􏰀nally􏰇), estimate the model performance on the testing data. Complete this section after everything else is done and you are ready to submit your work. Don't improve model after you have loaded testing data!\n",
    "\n",
    "1) Fit your NB model using the cross-validated optimal alpha using your complete work data (both training and validation). This is your best and 􏰀nal model.\n",
    "\n",
    "**THE BEST ALPHA WE FOUND IS 0.8 - WOHOO? LETS FIND OUT** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-8.21950568, -8.21950568, -7.04943442, ..., -9.03043589,\n",
       "        -7.77767292, -9.03043589]),\n",
       " array([-6.73854736, -8.53030683, -7.27754386, ..., -7.71937661,\n",
       "        -7.71937661, -7.71937661]))"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model for alpha = 0.8\n",
    "fitNB(train_clean.quote, train_clean.fresh, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Load your testing data. Clean it using exactly the same procedure (you made a function for this, right?) and transform it into BOW-s.\n",
    "\n",
    "Note: above I suggested using vectorizer.fit_transform(quote) function to create the BOW. Here I recommend to use vectorizer.transform(quote). This is because we don't want to change the vocabulary (that's what the fit-part does), only to transform it into the BOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test_rotten.csv\")\n",
    "#clean the test\n",
    "test_clean = clean(test_df)\n",
    "\n",
    "y_output = test_clean.fresh\n",
    "y_output = [1 if i == 'fresh' else 0 for i in y_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Predict the F/R class on testing data. Compute accuracy. Present it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.7113594041 !\n"
     ]
    }
   ],
   "source": [
    "predicted = test_clean.quote.apply(predictNB).to_frame().quote\n",
    "predicted = list(predicted)\n",
    "\n",
    "output = [a == p for a,p in zip(predicted,y_output)]\n",
    "accuracy = float(numpy.sum(output))/ float(len(output)) * 100\n",
    "\n",
    "print \"Accuracy:\", accuracy, \"!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Did you get a better or worse result compared to the k-NN and TF-IDF in PS04?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YES, WE GOT A BETTER RESULT!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is it. This is your 􏰀nal model performance measure. Feel free to compare it with your peers, but even if abysmal, don't play with the model any more! Just submit, and you are done 􏰇... really done, this was your last problem set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
