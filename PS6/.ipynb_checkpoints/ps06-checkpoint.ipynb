{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  \n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and split it into working and testing chunks. But before you begin: ensure you can save a\n",
    "dataframe in a format you can load back in afterwards. pd.to_csv is a good bet, but it has a lot of\n",
    "options which may screw up the way you read data. Ensure you can store data in a way that you can\n",
    "read it back in correctly, including that missings remain missings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. create a tiny toy data frame that includes some numbers, strings, and missings. Save it and ensure you can reload it in the correct form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john</td>\n",
       "      <td>18</td>\n",
       "      <td>student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jane</td>\n",
       "      <td>6</td>\n",
       "      <td>farmer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bob</td>\n",
       "      <td>36</td>\n",
       "      <td>barista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>candace</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name   age occupation\n",
       "0     john    18    student\n",
       "1     jane     6     farmer\n",
       "2      bob    36    barista\n",
       "3  candace  None       None"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"name\": [\"john\", \"jane\", \"bob\", \"candace\"], \"age\": [18, 6, 36], \"occupation\": [\"student\", \"farmer\", \"barista\"]}\n",
    "tiny_df = pd.DataFrame.from_dict(data, orient='index').T\n",
    "tiny_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tiny_df.to_csv(\"tiny_df.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john</td>\n",
       "      <td>18.0</td>\n",
       "      <td>student</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jane</td>\n",
       "      <td>6.0</td>\n",
       "      <td>farmer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bob</td>\n",
       "      <td>36.0</td>\n",
       "      <td>barista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>candace</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name   age occupation\n",
       "0     john  18.0    student\n",
       "1     jane   6.0     farmer\n",
       "2      bob  36.0    barista\n",
       "3  candace   NaN        NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"tiny_df.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are good to go:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. load the data (available on canvas: files/data/rotten-tomatoes.csv). DO NOT LOOK AT IT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tomatoes = pd.read_csv(\"reviews.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. split the dataset into working-testing parts (80/20 or so). Note that sklearn's train_test_split can easily handle dataframes. Just for your confirmation, ensure that the size of the working and testing data look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tomatoes.loc[:, tomatoes.columns != \"fresh\"], \n",
    "    tomatoes[\"fresh\"], \n",
    "    test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train features size: 86024\n"
     ]
    }
   ],
   "source": [
    "print(\"train features size: \" + str(X_train.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-cfd28c0d5e8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test features size: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"test features size: \" + str(X_test.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. now save the test data and delete it from memory. Use python's del statement, or R-s rm function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test.to_csv(\"X_test\", sep=',', index=False)\n",
    "y_test.to_csv(\"y_test\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X_test\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Explore and clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when the test data is put aside, we can breath out and take a closer look how does the work data look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Take a look at a few lines of data (you may use pd.sample for this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = X_train\n",
    "train_data[\"fresh\"] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic</th>\n",
       "      <th>imdb</th>\n",
       "      <th>link</th>\n",
       "      <th>publication</th>\n",
       "      <th>quote</th>\n",
       "      <th>review_date</th>\n",
       "      <th>rtid</th>\n",
       "      <th>title</th>\n",
       "      <th>fresh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6081</th>\n",
       "      <td>Roger Moore</td>\n",
       "      <td>482606</td>\n",
       "      <td>http://www.orlandosentinel.com/entertainment/m...</td>\n",
       "      <td>Orlando Sentinel</td>\n",
       "      <td>Fans of the 'pitiless/merciless killers' schoo...</td>\n",
       "      <td>2008-05-30 00:00:00</td>\n",
       "      <td>678555400</td>\n",
       "      <td>The Strangers</td>\n",
       "      <td>fresh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10165</th>\n",
       "      <td>Rick Groen</td>\n",
       "      <td>403217</td>\n",
       "      <td>http://www.theglobeandmail.com/servlet/story/R...</td>\n",
       "      <td>Globe and Mail</td>\n",
       "      <td>Gus Van Sant ventures into the valley of death...</td>\n",
       "      <td>2005-08-12 00:00:00</td>\n",
       "      <td>12868</td>\n",
       "      <td>Last Days</td>\n",
       "      <td>fresh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4546</th>\n",
       "      <td>David Ansen</td>\n",
       "      <td>117318</td>\n",
       "      <td>http://www.msnbc.com/m/nw/a/m/mv_p.asp#The%20P...</td>\n",
       "      <td>Newsweek</td>\n",
       "      <td>A brave, spectacularly entertaining -- and une...</td>\n",
       "      <td>2008-06-30 00:00:00</td>\n",
       "      <td>15475</td>\n",
       "      <td>The People Vs. Larry Flynt</td>\n",
       "      <td>fresh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9933</th>\n",
       "      <td>Jeff Millar</td>\n",
       "      <td>127722</td>\n",
       "      <td>http://www.chron.com/cs/CDA/moviestory.mpl/ae/...</td>\n",
       "      <td>Houston Chronicle</td>\n",
       "      <td>A well-executed scene can be followed by anoth...</td>\n",
       "      <td>2005-07-21 00:00:00</td>\n",
       "      <td>17297</td>\n",
       "      <td>Another Day In Paradise</td>\n",
       "      <td>rotten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3290</th>\n",
       "      <td>Hal Hinson</td>\n",
       "      <td>116320</td>\n",
       "      <td>http://www.washingtonpost.com/wp-srv/style/lon...</td>\n",
       "      <td>Washington Post</td>\n",
       "      <td>On the whole, Baldwin seems pretty dim for a r...</td>\n",
       "      <td>2002-01-22 00:00:00</td>\n",
       "      <td>16068</td>\n",
       "      <td>Fled</td>\n",
       "      <td>rotten</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            critic    imdb                                               link  \\\n",
       "6081   Roger Moore  482606  http://www.orlandosentinel.com/entertainment/m...   \n",
       "10165   Rick Groen  403217  http://www.theglobeandmail.com/servlet/story/R...   \n",
       "4546   David Ansen  117318  http://www.msnbc.com/m/nw/a/m/mv_p.asp#The%20P...   \n",
       "9933   Jeff Millar  127722  http://www.chron.com/cs/CDA/moviestory.mpl/ae/...   \n",
       "3290    Hal Hinson  116320  http://www.washingtonpost.com/wp-srv/style/lon...   \n",
       "\n",
       "             publication                                              quote  \\\n",
       "6081    Orlando Sentinel  Fans of the 'pitiless/merciless killers' schoo...   \n",
       "10165     Globe and Mail  Gus Van Sant ventures into the valley of death...   \n",
       "4546            Newsweek  A brave, spectacularly entertaining -- and une...   \n",
       "9933   Houston Chronicle  A well-executed scene can be followed by anoth...   \n",
       "3290     Washington Post  On the whole, Baldwin seems pretty dim for a r...   \n",
       "\n",
       "               review_date       rtid                       title   fresh  \n",
       "6081   2008-05-30 00:00:00  678555400               The Strangers   fresh  \n",
       "10165  2005-08-12 00:00:00      12868                   Last Days   fresh  \n",
       "4546   2008-06-30 00:00:00      15475  The People Vs. Larry Flynt   fresh  \n",
       "9933   2005-07-21 00:00:00      17297     Another Day In Paradise  rotten  \n",
       "3290   2002-01-22 00:00:00      16068                        Fled  rotten  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. print out all variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['critic', 'imdb', 'link', 'publication', 'quote', 'review_date', 'rtid',\n",
       "       'title', 'fresh'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. create a summary table (maybe more like a bullet list) where you print out the most important summary statistics for the most interesting variables. The most interesting facts you should present should include: a) number of missings for fresh and quote; b) all different values for fresh/rotten evaluations; c) counts or percentages of these values; d) number of zero-length or only whitespace quote-s; e) minimum-maximum-average length of quotes (either in words, or in characters). (Can you do this as an one-liner?); f) how many reviews are in data multiple times. Feel free to add more figures you consider relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nulls for fresh: 0\n",
      "Number of nulls for quote: 0\n",
      "\n",
      "Unique values for fresh/rotten: \n",
      "['fresh' 'rotten' 'none']\n",
      "\n",
      "Counts for unique values of fresh/rotten: \n",
      "fresh     6736\n",
      "rotten    3998\n",
      "none        19\n",
      "Name: fresh, dtype: int64\n",
      "\n",
      "Percentages for unique values of fresh/rotten: \n",
      "fresh     62.642983\n",
      "rotten    37.180322\n",
      "none       0.176695\n",
      "Name: fresh, dtype: float64\n",
      "\n",
      "Number of zero-length or whitespace quotes: 0\n",
      "\n",
      "Minimum, maximum, and average length of quotes in words: 1, 49, 20.108992839207662\n",
      "\n",
      "Reviews in data multiple times: 369\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of nulls for fresh: \" + str(y_train.isnull().sum()))\n",
    "print(\"Number of nulls for quote: \" + str(X_train[\"quote\"].isnull().sum(axis = 0)))\n",
    "print()\n",
    "print(\"Unique values for fresh/rotten: \")\n",
    "print(y_train.unique())\n",
    "print()\n",
    "print(\"Counts for unique values of fresh/rotten: \")\n",
    "print(y_train.value_counts())\n",
    "print()\n",
    "print(\"Percentages for unique values of fresh/rotten: \")\n",
    "print(y_train.value_counts()/y_train.size*100)\n",
    "print()\n",
    "print(\"Number of zero-length or whitespace quotes: \" + str(X_train.loc[X_train[\"quote\"] == \"\"].size + X_train.loc[X_train[\"quote\"].str.isspace()].size))\n",
    "print()\n",
    "print(\"Minimum, maximum, and average length of quotes in words: \" + str(X_train[\"quote\"].str.split().apply(len).min()) + \", \" + str(X_train[\"quote\"].str.split().apply(len).max()) + \", \" + str(X_train[\"quote\"].str.split().apply(len).mean()))\n",
    "print()\n",
    "print(\"Reviews in data multiple times: \" + str(len(X_train[X_train.duplicated() == True])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Now when you have an overview what you have in data, clean it by removing all the inconsistencies the table reveals. We have to ensure that the central variables: quote and fresh are not missing, and quote is not an empty string (or just contain spaces and such). I strongly recommend to do it as a standalone function because at the end you have to perform exactly the same cleaning operations with your test data too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    data.dropna(subset=['fresh', 'quote'], inplace=True)\n",
    "    data = data[data[\"fresh\"] != \"none\"]\n",
    "    data = data[data[\"quote\"] != \"\"]\n",
    "    data = data[data[\"quote\"].str.isspace() == False]\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    return data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train_data = clean(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now where you are familiar with the data, it's time to get serious and implement the Naive Bayes classi\u001c",
    "er\n",
    "from scratch. But first things first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Ensure you are familiar with Naive Bayes. Consult the readings, available on canvas. Schutt & O'Neill is an easy and accessible (and long) introduction, Whitten & Frank is a lot shorter but still accessible introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Convert your data (quotes) into bag-of-words. Your code should look something along the lines as in PS4. However, now we don't want BOW that contains counts of words in quotes, but just 1/0 (or true/-false) for the presence/non-presence of the words. Convert the count-based BOW into such a presence BOW. Hint: think in terms of vectorized (universal) functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(cleaned_train_data[\"quote\"]).toarray()\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Split your work data and target (i.e. the variable fresh) into training and validation chunks (80/20 or so). Later we also do cross-validation, but for now, a simple training/validation will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_t, X_v, y_t, y_v = train_test_split(\n",
    "    cleaned_train_data.loc[:, cleaned_train_data.columns != \"fresh\"], \n",
    "    cleaned_train_data[\"fresh\"], \n",
    "    test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good. Now you are ready with the preparatory work and it's time to dive into the real thing. Let's\n",
    "implement Naive Bayes. Use only training data in the fitting below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Compute the unconditional (log) probability that the tomato is fresh/rotten, log Pr(F), and log Pr(R). These probabilities are based on the values of fresh variable but not on the words the quotes contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log probability that tomato is fresh: -0.469399582112\n",
      "log probability that tomato is rotten: -0.981836809772\n"
     ]
    }
   ],
   "source": [
    "Nfresh = y_t[y_t == \"fresh\"].count()\n",
    "Nrotten = y_t[y_t == \"rotten\"].count()\n",
    "Ntotal = Nfresh + Nrotten\n",
    "Pfresh = Nfresh/Ntotal\n",
    "Protten = Nrotten/Ntotal\n",
    "print(\"log probability that tomato is fresh: \" + str(np.log(Pfresh)))\n",
    "print(\"log probability that tomato is rotten: \" + str(np.log(Protten)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. For each word w, compute log Pr(w|F) and log Pr(w|R), the (log) probability that the word is present in a fresh/rotten review. These probabilities can easily be calculated from counts of how many times these words are present for each class. Hint: these computations are based on your BOW-s X. Look at ways to sum along columns in this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability words are in fresh review: [-5.98664526 -5.55586234 -6.47215308 ..., -5.60715564 -5.55586234\n",
      " -5.60715564]\n",
      "Probability words are in rotten review: [-4.82028157 -4.86110356 -6.65286303 ..., -5.40010006 -5.09471841\n",
      " -5.09471841]\n"
     ]
    }
   ],
   "source": [
    "f = X[y_t[y_t == \"fresh\"].index,:]\n",
    "r = X[y_t[y_t == \"rotten\"].index,:]\n",
    "PwordsF = np.sum(f, axis=1)/Nfresh\n",
    "PwordsR = np.sum(r, axis=1)/Nrotten\n",
    "LPF = np.log(PwordsF)\n",
    "LPR = np.log(PwordsR)\n",
    "print(\"Probability words are in fresh review: \" + str(LPF))\n",
    "print(\"Probability words are in rotten review: \" + str(LPR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are done with the estimator. Your fitted model is completely described by these four probability\n",
    "vectors: log Pr(F), log Pr(R), log Pr(w|F), log Pr(w|R). Let's now turn to prediction, and pull out your\n",
    "validation data (not the test data!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. For both destination classes, F and R, compute the log-likelihood that the quote belongs to this class. Based on the log-likelihoods, predict the class F or R for each quote in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.apply_along_axis(lambda x: np.sum(), 1, x) + np.log(PFresh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Print the resulting confusion matrix and accuracy (feel free to use existing libraries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# confusion_matrix(y_v, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to look at your fitted model a little bit closer. NB model probabilities are rather easy to understand and interpret. The task here is to find the best words to predict a fresh, and a rotten review. And we only want to look at words that are reasonably frequent, say more frequent than 30 times in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Extract from your conditional probability vectors log Pr(F) and log Pr(R) the probabilities that correspond to frequent words only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequentF = np.sum(f, axis=1) > 30\n",
    "frequentR = np.sum(r, axis=1) > 30\n",
    "frequent_LPF = LPF[np.where(frequentF==True)]\n",
    "frequent_LPR = LPR[np.where(frequentR==True)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Find 10 best words to predict F and 10 best words to predict R. Hint: imagine we have a review that contains just a single word. Which word will give the highest weight to the probability the review is fresh? Which one to the likelihood it is rotten? Comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 best words to predict F: ['1957', '27', '1971', 'about', 'active', '1963', '20000', 'abroad', '90', '17th']\n"
     ]
    }
   ],
   "source": [
    "print(\"10 best words to predict F: \" + str([words[i] for i in (-frequent_LPF).argsort()[:10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 best words to predict R: ['1955', '1993', '136', '39', '8212', '21st', '231', '1920s', '3000', '130']\n"
     ]
    }
   ],
   "source": [
    "print(\"10 best words to predict R: \" + str([words[i] for i in (-frequent_LPR).argsort()[:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Print out a few missclassified quotes. Can you understand why these are misclassified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 NB with smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now you have your brand-new NB algorithm up and running. As a next step, we add smoothing to it. As you will be doing cross-validation below, your first task is to mold what you did above into two funcions: one for fitting and another one for predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Create two functions: one for fitting NB model, and another to predict outcome based on the fitted model. As mentioned above, the model is fully described with 4 probabilities, so your fitting function may return such a list as the model; and the prediction function may take it as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fitNB(X_t, y_t, alpha):\n",
    "    Nfresh = y_t[y_t == \"fresh\"].count()\n",
    "    Nrotten = y_t[y_t == \"rotten\"].count()\n",
    "    Ntotal = Nfresh + Nrotten\n",
    "    Pfresh = Nfresh/Ntotal\n",
    "    Protten = Nrotten/Ntotal\n",
    "    LPF = np.log(Pfresh)\n",
    "    LPR = np.log(Protten)\n",
    "    return Pfresh, Protten, LPF, LPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-29-84aceb6bec43>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-84aceb6bec43>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def predictNB():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Add smoothing to the model. See Schutt p 103 and 109. Smoothing amounts to assuming that we have \"seen\" every possible work α > 0 times already, for both classes. (If you wish, you can also assume you have seen the words α times for F and β times for R). Note that α does not have to be an integer, and typically the best α < 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Now fit a few models with different α-s and see if the accuracy improves compared to the baseline case above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally (well, almost finally), we do cross-validation. This is another piece of code you have to implement yourself, not use existing libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implement k-fold CV. I recommend to implement it as a function that a) puts your data into random order; b) splits these into k chunks; c) selects a chunk for testing and the others for training; d) trains your NB model on the training chunks; e) computes accuracy on training chunk; f) returns mean accuracy over all these k trials. The function should also take α as an argument, this is the hyperparameter you are going to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv(k):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find the optimal α by 5-fold CV using your own CV code. You have to find the cross-validated accuracies for a number of α-s between 0 and 1. Present the accuracy as a function of α on a plot and indicate which one is the best α."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Final model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally (and now I mean finally,), estimate the model performance on the testing data. Complete this section after everything else is done and you are ready to submit your work. Don't improve model after you have loaded testing data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Fit your NB model using the cross-validated optimal alpha using your complete work data (both training and validation). This is your best and final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Load your testing data. Clean it using exactly the same procedure (you made a function for this, right?) and transform it into BOW-s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Predict the F/R class on testing data. Compute accuracy. Present it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Did you get a better or worse result compared to the k-NN and TF-IDF in PS04?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
